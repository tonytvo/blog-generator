---
title: the right IT by Alberto Savoia summary
date: "2025-06-28T22:12:03.284Z"
description: "acquirer multiple by Tobias Carlisle summary"
tags: ["softwaredevelopment", "productmanagement"]
---

# ğŸ§¨ **WHY SO MANY IDEAS FAIL**

---

## ğŸ“‰ **The Law of Market Failure**

### ğŸ”º **â€œMost ideas fail in the marketâ€”even good ones.â€**

Alberto Savoia introduces one of the bookâ€™s cornerstone ideas: the **Law of Market Failure**.

> âœ… **â€œThe Law of Market Failure states that most new ideasâ€”regardless of how promising they seemâ€”will fail in the market.â€**

This is not just anecdotal; **decades of data** across industries show a **consistent 80% to 90% failure rate** for:

* Startups,
* New product launches,
* Marketing campaigns,
* Internal corporate innovations.

### âŒ **Not Technical Failure â€” Market Failure**

Most teams mistakenly believe that if they **build a good product**, customers will naturally come. But:

> **â€œThe market doesnâ€™t care how good your idea is if it doesnâ€™t solve a real problem.â€**

Even the **best engineered, beautifully designed** products fail **if nobody wants them.**

---

### ğŸ§ª **Three Types of Risk in Innovation**

Savoia breaks down innovation risks into 3 types:

1. **Technical Risk** â€“ Can we build it?
2. **Execution Risk** â€“ Can we deliver it on time, on budget?
3. **Market Risk** â€“ **Will they use/buy it?**

Most teams focus obsessively on the first twoâ€”and **neglect the most dangerous one: Market Risk.**

> âœ… **â€œIf the market doesnâ€™t want it, the rest doesnâ€™t matter.â€**

---

### ğŸ­ **The Illusion of Progress**

One of the most damaging traps is the **illusion of progress**. Teams believe they're making headway because they're:

* Hiring engineers,
* Holding design sprints,
* Completing prototypes,
* Writing code,
* Launching MVPsâ€¦

â€¦but theyâ€™re **not testing the core assumption**: **Do people actually want this?**

> ğŸ”¥ **â€œWe confuse activity with progress.â€**

---

### âš ï¸ Real-World Examples:

#### âŒ Google Wave (2009)

* **Massive hype**, elite engineering team, beautiful UI.
* But: **no one understood the problem it solved**.
* **Shut down after 1 year.**

#### âŒ Juicero

* High-tech juicer startup. Raised \$120M.
* Required proprietary juice packs. A **manual squeeze** did the same thing.
* Exposed by journalists. **Collapsed under ridicule.**

#### âœ… Dropbox

* Before building anything, the founders created a **demo video** explaining the product.
* **Measured signups and interest** before investing in infrastructure.
* **Pretotyped**, then built.

---

## ğŸ§  **Mindsets That Lead to Failure**

Savoia argues that idea failure is often **not a business problem, but a psychology problem**.

> âœ… **â€œWe donâ€™t fail because of poor execution. We fail because of self-deception.â€**

---

### 1ï¸âƒ£ **The Reality Distortion Field**

Coined at Apple to describe Steve Jobsâ€™ ability to bend perception, but when used by inexperienced innovators, it leads to disaster.

* **Founders fall in love** with their vision.
* They **filter out negative feedback**, dismiss critics, and overestimate demand.

> â— **â€œWhen you believe in your idea too much, you lose sight of reality.â€**

#### Example:

* A team builds an app to gamify learning Chinese.
* Early feedback: â€œToo gimmicky.â€
* They ignore it, saying users â€œjust need more exposure.â€
* After 6 months, usage drops to near-zero.

---

### 2ï¸âƒ£ **Overconfidence Bias**

> **â€œBecause we came up with the idea, we assume others will love it too.â€**

This bias leads to:

* Ignoring competitive research,
* Skipping market validation,
* Launching too soon.

#### Example:

* A founder says, *â€œIâ€™d use this, so everyone else will too.â€*
* But **you are not your customer.**
* Without evidence, this assumption is **a gamble, not a strategy.**

---

### 3ï¸âƒ£ **Confirmation Bias**

* Innovators selectively look for data that **supports** their idea.
* They ignore or rationalize **contradictory evidence**.

> **â€œWe become detectives searching for cluesâ€”only we ignore anything that doesnâ€™t fit our theory.â€**

#### Example:

* 100 people visit your fake landing page.
* 5 people sign up.
* You tell yourself: *â€œLook, 5% interest!â€*
* You ignore: *â€œ95% bouncedâ€”they didnâ€™t care.â€*

---

### 4ï¸âƒ£ **Emotional Attachment to the Idea**

> ğŸ’” **â€œThe more time you spend on an idea, the harder it is to let goâ€”even if itâ€™s wrong.â€**

This is called the **sunk cost fallacy**. Teams:

* Keep tweaking,
* Keep iterating,
* Keep believing...

...instead of **testing core assumptions** or **killing the idea early**.

---

### âœ… **The Better Mindset: Fall in Love With the Problem**

One of the most important quotes in the book:

> ğŸ’¡ **â€œFall in love with the problem, not the solution.â€**

* Stay **obsessed with solving a pain** or fulfilling a need.
* Be **open to changing your solution**â€”or abandoning itâ€”if it doesnâ€™t serve that goal.
* Innovation is **not about being right from the start**, but about **discovering whatâ€™s right** through experimentation.

---

### ğŸ”„ RECAP: Why Most Ideas Fail

| Trap                     | Why It Happens                            | What to Do Instead                                          |
| ------------------------ | ----------------------------------------- | ----------------------------------------------------------- |
| **Assuming demand**      | You think â€œIf I build it, they will comeâ€ | Use **pretotyping** to test demand before building          |
| **Illusion of progress** | You celebrate activity, not results       | Measure **engagement, not effort**                          |
| **Overconfidence**       | You trust your gut too much               | Demand **external evidence**                                |
| **Confirmation bias**    | You only see the good signals             | Track **all user behavior**, not just highlights            |
| **Emotional attachment** | Youâ€™ve invested too much to let go        | Remember: **Killing a bad idea early saves time and money** |



Certainly! Here's an **even deeper, expanded explanation** of **Part 1: The Product Fallacy** from *"Evidence-Guided" by Itamar Gilad*, with bold-highlighted key phrases, layered reasoning, more case studies, and actionable insights:

---

## ğŸ§¨ **PART 1: THE PRODUCT FALLACY** â€” *Why Our Product Thinking Is Broken*

This section sets the **critical foundation** for the book, asserting that **most product teams operate under flawed assumptions** about how successful products are created. Gilad dismantles these myths with psychological insight, empirical evidence, and industry case studies.

---

### ğŸ§  **1. The Product Fallacy â€“ The Myth That Keeps Teams Busy But Ineffective**

#### ğŸ”® **The Traditional Belief**

> **â€œSuccess = a brilliant idea + a skilled team + fast execution.â€**

This formula is taught, rewarded, and reinforced across companiesâ€”especially in tech. However, itâ€™s **seductively simple** and deeply **misleading**.

---

#### ğŸ’¥ **Reality: The Link Between Idea and Success is Weak**

> **â€œIn reality, success is highly uncertain, and most ideas do not work.â€**

Even the **best teams** cannot escape the fundamental problem: **we are bad at predicting what users want**.

ğŸ“Œ **Example**:
Google launched Google Wave (a communication platform), backed by a superstar team and hype.
ğŸ”» **Outcome**: It failed spectacularly because **users didnâ€™t understand its value**, despite internal belief it would revolutionize communication.

---

#### ğŸ§  **The Role of Cognitive Biases**

Gilad connects **systematic product failure** to **cognitive psychology**â€”our brains are wired in ways that **skew judgment** and inflate confidence.

**Key Biases:**

1. **Overconfidence Bias**

   > **â€œWe tend to be far too certain in the accuracy of our beliefs.â€**
   > PMs often overrate their product intuition and downplay the unknowns.

2. **Confirmation Bias**

   > **â€œOnce we believe something, we seek evidence to support it and ignore the rest.â€**
   > Teams selectively cherry-pick customer quotes, metrics, or anecdotes that validate their pet ideas.

3. **Survivorship Bias**

   > **â€œWe focus only on the winners and ignore the graveyard of failed products.â€**
   > Most books, talks, and blog posts feature success stories. We don't hear about the hundreds of failed apps, A/B tests, or product pivots that died quietly.

ğŸ“Œ **Case Study**:
Facebookâ€™s "Stories" format succeeded after copying Snapchat. But Facebook had previously launched and killed many storytelling and ephemeral content features. The public sees only the **winning version**, not the **failed experiments**.

---

### âŒ **2. Most Ideas Fail â€“ And Thatâ€™s Normal**

#### ğŸ“‰ **The Data Is Stark**

> **â€œAcross the industry, 70%â€“90% of ideas produce little to no value.â€**

This isnâ€™t a one-time anomalyâ€”itâ€™s a **repeatable trend across teams, products, and decades**.

ğŸ“Š **Supporting Evidence**:

* **Microsoft**: Only \~1 in 3 ideas tested improved key metrics.
* **Google**: Ran thousands of experimentsâ€”**most ideas performed no better or worse** than the status quo.
* **Booking.com**: Heavily data-driven and experimentalâ€”**only 10-15% of tests result in significant positive change**.

---

#### ğŸ§ª **The Experimentation Backlog**

> **â€œThe product roadmap is not a delivery queueâ€”itâ€™s a hypothesis list.â€**

If most ideas fail, we should not treat them as **projects to build**, but as **guesses to test**.

ğŸ“Œ **Real-World Insight**:
Instead of saying: â€œWe will launch Feature A in Q2,â€ say:
ğŸ” â€œWe believe Feature A may solve Problem X, and we will test it via a prototype or limited release before scaling.â€

---

### ğŸ² **3. Our Ideas Are Mostly Guesses â€“ And Thatâ€™s Okay**

#### ğŸš§ **All Ideas Are Assumptions Until Proven Otherwise**

> **â€œYour roadmap is a graveyard of guesses.â€**

Gilad emphasizes that **ideation** is inherently **speculative**. Great product thinking isnâ€™t about being right from the startâ€”itâ€™s about being **adaptive and humble**.

---

#### ğŸ’¬ **The Expert Myth**

> **â€œEven the most experienced product leaders are wrong most of the time.â€**

Experts have **pattern recognition**, but patterns donâ€™t guarantee correctness. Markets change. Contexts shift. **You need evidence.**

ğŸ“Œ **Example**:
At Google, even senior engineers and PMs often failed A/B tests they were confident in. Over time, this **eroded reliance on opinion** and built a culture of **testing everything**.

---

#### ğŸ” **HiPPO Decision-Making: A Red Flag**

> **â€œWhen the Highest Paid Personâ€™s Opinion overrides evidence, the team is flying blind.â€**

To shift to better product thinking:

* Replace **opinions** with **observations**.
* Replace **authority** with **user insight**.
* Replace **roadmaps** with **evidence ladders** (discussed later in the book).

---

### ğŸš€ **4. Output â‰  Outcome â€“ The Root of False Productivity**

#### âš™ï¸ **Output: Building for the Sake of Delivery**

> **â€œMany teams mistake activity for progress.â€**

Output is:

* Features shipped
* Sprints completed
* Code written

But none of these **guarantee value** to users or business.

---

#### ğŸ“ˆ **Outcome: Real, Measurable Impact**

> **â€œOutcome is the real north star: changes in user behavior that create value.â€**

Examples:

* Increase in daily active users (DAU)
* Reduced churn
* Higher customer satisfaction
* Increased conversion rate

ğŸ“Œ **Contrast Example**:

| Metric      | Output                  | Outcome                                     |
| ----------- | ----------------------- | ------------------------------------------- |
| Feature     | Rolled out dark mode    | 40% of users enable it & use app 10% longer |
| Performance | Shipped faster checkout | Conversion rate rises by 15%                |
| Marketing   | Sent newsletter         | Open rates rise; reactivation improves      |

---

#### ğŸ­ **Vanity Metrics: Dangerous Illusions**

> **â€œJust because itâ€™s measurable doesnâ€™t mean itâ€™s meaningful.â€**

Examples:

* # of tickets closed
* Story points completed
* Number of deployments

These **donâ€™t correlate** with user or business value. They can make teams feel good, but **hide real problems**.

---

#### ğŸ” **Shipping â‰  Success**

Gilad warns of the **release trap**â€”celebrating launches while ignoring results.

ğŸ“Œ **Better Practice**:
Measure adoption, usage, behavior change **after launch**. Treat releases as **experiments**, not finish lines.

---

## ğŸ§  **Summary of Key Shifts in Thinking (Mental Model Shift)**

| Old Paradigm                         | Evidence-Guided Paradigm                         |
| ------------------------------------ | ------------------------------------------------ |
| Ideas are facts                      | Ideas are **guesses** to be tested               |
| Experts know what works              | Experts also need **validation**                 |
| Shipping = success                   | **Impact = success**                             |
| Measure speed and volume (output)    | Measure **value and outcome**                    |
| Decide based on opinion or authority | Decide based on **evidence and experimentation** |

---


# âš™ï¸ MAKE SURE YOU HAVE THE RIGHT IT

---

## ğŸ” **Pretotyping vs. Prototyping**

### ğŸ§  **"Donâ€™t just build it right. First, make sure youâ€™re building the right 'It'."**

One of the **most dangerous myths** in product development is the idea that **if you build a great product, customers will come**. This chapter breaks that myth by contrasting **prototyping** with a more critical, often ignored step: **pretotyping**.

---

### ğŸ§ª **What Is a Pretotype?**

> **"Pretotyping is about testing the marketâ€™s genuine interest in your ideaâ€”before you build anything real or expensive."**

Pretotyping helps you **fail fast and cheaply**, so you donâ€™t **succeed at building something no one wants.**

**Pretotype = Pre + Prototype**
Itâ€™s **not a half-built product**â€”itâ€™s a **simulation or illusion** designed to answer the only question that matters early on:

> â“ **"Will they use it if we build it?"**

---

### ğŸ”§ **What Is a Prototype?**

> **"A prototype answers the question: â€˜Can we build it?â€™"**

Itâ€™s about testing features, functionality, usability, design, etc. It assumes youâ€™ve already validated market interestâ€”which is often **not the case**.

---

### ğŸ“Š Why Pretotyping Must Come First

Savoia insists:

> âœ… **â€œIt is far cheaper to test an ideaâ€™s desirability than to assume it and risk full development.â€**

---

### ğŸš€ Real-World Case Examples:

#### âœ… **Zappos (Shoes Online)**

* Founder **Nick Swinmurn** didnâ€™t build an e-commerce system first.
* He went to local shoe stores, took **photos**, and listed them online.
* When someone ordered, he went and bought the shoes manually.

ğŸ‘‰ **Pretotyping**: Validated **"Would people buy shoes online?"**

---

#### âŒ **Segway**

* \$100M+ invested in development.
* World-class design and tech.
* Assumed it would **revolutionize transportation**.
* Reality: **No one knew where to ride it or wanted to change habits.**

ğŸ‘‰ They **prototyped** brilliantly, but **never pretotyped**.

---

## ğŸ”§ **The Right It Tools & Metrics**

This chapter introduces a **framework of behavioral hypotheses and metrics** to help you know if youâ€™re on the path toward **The Right It**. You donâ€™t have to guessâ€”you test with data.

---

### ğŸ“Œ **1. The XYZ Hypothesis**

> ğŸ“ **"X people will do Y within Z time."**

This is the core of every pretotypeâ€”it forces you to be specific, measurable, and accountable.

---

#### ğŸ” Why Itâ€™s Powerful:

* Prevents **vague hopes** like â€œpeople will love this.â€
* Encourages **clear, testable predictions**.

#### âœ… Example:

> â€œ**500 people will click the â€˜Join Waitlistâ€™ button for our new budgeting app within 5 days**.â€

#### âŒ Bad version:

> â€œPeople will probably be interested in our app.â€ (No numbers, no time frame, no behavior.)

---

### ğŸ§ª **How to Use XYZ Hypotheses Effectively:**

* **X** = How many people?
* **Y** = What action shows interest? (click, sign up, preorderâ€¦)
* **Z** = In what time frame?

> âœ… **â€œIf you canâ€™t test it in time and with numbers, itâ€™s not a real hypothesis.â€**

---

### ğŸ§¬ **2. Market Engagement Hypothesis (MEH)**

> **â€œThe MEH is where your XYZ Hypothesis meets reality.â€**

This is your assumption about how the market will respond when given the **opportunity to act**, even with **no real product yet**.

---

### ğŸ§ª How to Run a MEH Test:

* Use a **landing page** with a fake offer and a CTA (Buy Now, Join Beta).
* Use **ads** to see if people click on a fake product.
* Track real user behavior, not just traffic or likes.

#### âœ… Example:

Create a simple site:

> â€œNew App: StudyTime â€” Beat Procrastination. Sign up for early access.â€
> ğŸ‘‰ Measure signups (Y) over 5 days (Z) from 1,000 visitors (X).

---

### ğŸ“Š **3. Initial Level of Interest (ILI)**

> **"ILI = People who take action / People exposed to the test."**

This is your **conversion rate**, and it's a direct signal of potential market demand.

#### âœ… Example:

* 1,000 visitors.
* 80 clicked "Sign Up".
* ğŸ‘‰ **ILI = 8%**

You can now **compare** this against your target. If your goal was **5%**, then 8% is strong validation.

> ğŸ“ˆ **"ILI transforms qualitative ideas into quantitative traction."**

---

### ğŸ” **Whatâ€™s a Good ILI?**

Savoia doesn't give hard rulesâ€”it varies by marketâ€”but generally:

* **>10%** = promising traction.
* **5â€“10%** = further testing or iteration needed.
* **<5%** = likely weak demand.

> ğŸš« **â€œDonâ€™t chase unicorns with limp engagement metrics.â€**

---

### ğŸ‘©â€ğŸ”¬ **4. High-Expectation Customers (HXC)**

> ğŸ§  **â€œThe best feedback doesnâ€™t come from average usersâ€”it comes from the most demanding ones.â€**

**HXCs** are:

* Already searching for a solution,
* Deeply understand the problem,
* Hard to impress.

But if **you win them**, you're likely building something great.

---

### ğŸ¯ Why Target HXCs Early?

* They provide **blunt, high-signal feedback**.
* If they **adopt** your pretotype, it's a green light.
* If they **ignore** or **criticize** it, you're in danger.

#### âœ… Example: Launching a writing AI tool

* Your HXCs = **daily writers**, **bloggers**, **content creators**.
* Place your fake landing page in **Reddit/r/copywriting** or a writers' forum.
* Monitor engagement and qualitative comments.

> âœ… **â€œIf the people who need it most donâ€™t care, why would the general public?â€**

---

## ğŸ§­ **Mindset and Measurement Shift: The New Way to Innovate**

| Old Mindset            | The Right It Mindset                     |
| ---------------------- | ---------------------------------------- |
| Guess and build        | **Test and measure**                     |
| Listen to opinions     | **Watch actual behavior**                |
| Invest early           | **Validate early, build later**          |
| Seek praise            | **Seek honest disinterest or rejection** |
| Perfect your prototype | **Perfect your XYZ and MEH tests first** |

---

## âœ… Summary: The Right It Toolkit in Practice

| Tool               | Purpose                                 | What It Reveals                               |
| ------------------ | --------------------------------------- | --------------------------------------------- |
| **XYZ Hypothesis** | Define your expected market behavior    | Are you making a testable prediction?         |
| **MEH**            | Run a quick, fake or simulated test     | Will people take action now?                  |
| **ILI**            | Quantify initial traction               | How strong is early interest?                 |
| **HXC**            | Stress-test your idea on early adopters | Is it compelling to the most demanding users? |

> ğŸš€ **â€œIf you canâ€™t get people to engage with a fake version of your product, they probably wonâ€™t care when itâ€™s real.â€**

---

Absolutely! Below is a **comprehensive and expanded version** of:

# âš™ï¸ **TESTING YOUR IDEA â€“ THE PRETOTYPE METHOD**

From *The Right It* by **Alberto Savoia**, complete with **bold-highlighted principles**, **deep explanations**, and **real-world, practical examples**.

---

## ğŸ§° OVERVIEW: Why You Need Pretotyping

After understanding **why most ideas fail** (Part 1) and learning **how to define and frame a testable idea** (Part 2), this part focuses on **how to actually test it**â€”quickly, cheaply, and **before building anything real**.

> ğŸ’¡ **â€œA pretotype is not a lesser version of your productâ€”itâ€™s a smarter version of your decision-making process.â€**

Youâ€™ll learn to **simulate usage**, **observe behavior**, and **collect real engagement data** with minimal investment.

---

## ğŸ› ï¸ **The Six Pretotype Methods**

Each pretotype method is a **tactical technique** that allows you to test a specific aspect of user interest or behavior with **minimal effort**.

---

### ğŸ”¹ **1. The Fake Door Test**

> ğŸšª **â€œPut up a door. If no one tries to open it, donâ€™t build the house.â€**

#### âœ… What it is:

You create a **fake entry point** (e.g., a signup button, a â€œBuy Nowâ€ page) for a product or feature that **doesnâ€™t exist yet**.

#### ğŸ¯ Goal:

Measure **user intent** through **clicks, signups, or interest**, without having to build anything.

#### ğŸ§ª How it works:

* Build a **landing page or CTA** for your idea.
* When users click, show a message like:

  > â€œThanks! Weâ€™re not quite ready yet. Join the waitlist and weâ€™ll notify you soon!â€

#### ğŸ§  Example:

A new budgeting app idea â†’

* You run Google Ads with â€œTame Your Spending with BudgetBuddy.â€
* Visitors click â€œTry It Freeâ€ on a basic page.
* You measure clicks, email captures, bounce rate.

> âœ… **If no one clicks the button, why build the product?**

---

### ğŸ”¹ **2. The Infiltrator Test**

> ğŸ•µï¸ **â€œSlip your idea into an existing environment and watch what happens.â€**

#### âœ… What it is:

Insert your product idea **into an existing system**, platform, or experience **without fanfare**â€”and see if people use it naturally.

#### ğŸ¯ Goal:

Validate **real-world fit and organic adoption**.

#### ğŸ§ª How it works:

* Add a small new button, feature, or prompt inside an existing product, store, or page.
* Don't promote itâ€”let it sit.
* Track interactions or neglect.

#### ğŸ§  Example:

You want to test a feature that suggests AI-generated job descriptions.
You quietly embed a â€œTry SmartWriterâ€ button inside your existing HR dashboard.

* Measure how many users discover and click it **organically**.

> ğŸ’¡ **â€œIf users donâ€™t notice or use it when itâ€™s in their flow, it might not matter.â€**

---

### ğŸ”¹ **3. The One-Night Stand Test**

> ğŸŒ™ **â€œOffer your product temporarilyâ€”because permanent commitment is expensive.â€**

#### âœ… What it is:

Provide your product or service for **just one day** (or a limited window) and observe demand and behavior.

#### ğŸ¯ Goal:

Quickly simulate real-world conditions with **short-term exposure** and **no long-term risk**.

#### ğŸ§ª How it works:

* Launch a **popup**, **pop-up store**, **webinar**, or **flash offer**.
* Announce it through minimal ads, email, or social.
* Measure engagement: signups, show-ups, purchases.

#### ğŸ§  Example:

You want to launch a fitness accountability program.

* Offer a **1-day trial event** for \$5 with a live coach on Zoom.
* Track how many sign up, how many show up, how they react.

> ğŸ” **â€œA one-night stand reveals if thereâ€™s chemistryâ€”before you commit to marriage.â€**

---

### ğŸ”¹ **4. The Pinocchio Test**

> ğŸªµ **â€œBuild a fake product that looks realâ€”but doesnâ€™t function.â€**

#### âœ… What it is:

You create a **non-functional mockup** of your product to **observe user interaction** and test comprehension or desirability.

#### ğŸ¯ Goal:

Find out if people **understand and engage** with your product concept, even if it doesnâ€™t work.

#### ğŸ§ª How it works:

* Create a fake version: a dummy app screen, clickable prototype, 3D printed device shell, etc.
* Give it to users.
* Ask them to â€œuse itâ€ as if it were real.

#### ğŸ§  Example:

Testing an â€œAI-powered grocery list planner.â€

* Create a Figma prototype of the app.
* Watch users try to interact.
* Do they know what it does? Where they click first? Do they seem excited or bored?

> âœ… **â€œIf they donâ€™t want the fake version, they wonâ€™t want the real one either.â€**

---

### ğŸ”¹ **5. The Mechanical Turk**

> ğŸ§  **â€œSimulate the output of a machineâ€”using human effort behind the scenes.â€**

#### âœ… What it is:

You **fake a software feature or hardware automation** by **doing the task manually**, while making it look automated.

#### ğŸ¯ Goal:

Test if people will use the functionality **before building complex tech**.

#### ğŸ§ª How it works:

* You create a front-end or interface that mimics the full product.
* Behind the scenes, your team fulfills requests by hand.

#### ğŸ§  Example:

You want to build â€œAI Resume Booster.â€

* Users upload resumes and get edits back.
* Instead of AI, your editor does the work manually.
* You test demand **before building the AI model.**

> ğŸ’¡ **â€œWhy code an algorithm until you know theyâ€™ll pay for the outcome?â€**

---

### ğŸ”¹ **6. The Re-label Test**

> ğŸ” **â€œTake an old product. Give it a new name or use case. See what happens.â€**

#### âœ… What it is:

You test new positioning or audience segments by **repackaging an existing product**.

#### ğŸ¯ Goal:

Test if a **new use case, niche, or branding** resonates better.

#### ğŸ§ª How it works:

* Use your current product or someone elseâ€™s.
* Market it to a new group, with new messaging.
* Track conversion and interest.

#### ğŸ§  Example:

You already sell sleep headphones to travelers.
Re-label them:

> â€œ**Deep Focus Headphones for Remote Developers**â€
> Run new ads. See which segment responds better.

> ğŸ“ˆ **â€œSometimes the product isnâ€™t wrongâ€”the audience is.â€**

---

## ğŸ“ **Innovation Metrics & Decision Frameworks**

> ğŸ“Š **â€œPretotyping without measurement is just theater.â€**

After you run your pretotyping tests, how do you know if your idea is â€œThe Right Itâ€? You need a **decision system based on behavior, not bias**.

---

### ğŸ§  Key Innovation Metrics

#### ğŸ”¸ **XYZ Hypothesis**

> **â€œX people will do Y within Z time.â€**

This turns vague ideas into specific predictions.

#### ğŸ”¸ **MEH â€“ Market Engagement Hypothesis**

> **â€œIf we offer this, people will do that.â€**
> The bridge between concept and test.

#### ğŸ”¸ **ILI â€“ Initial Level of Interest**

> **ILI = (# who take action) / (Total exposed)**

A clean metric to **quantify real engagement**.

---

### ğŸ” What to Do With the Data

| Outcome            | Interpretation        | Decision            |
| ------------------ | --------------------- | ------------------- |
| High ILI (10â€“15%+) | Strong early interest | âœ… Go ahead with MVP |
| Medium ILI (5â€“10%) | Mixed signals         | ğŸ” Iterate or retest |
| Low ILI (<5%)      | Weak or no demand     | âŒ Kill the idea     |

---

### ğŸ’¥ Focus on Behavior, Not Applause

> âŒ Donâ€™t fall for **vanity metrics** like:

* Pageviews
* Likes
* Comments
* Shares

> âœ… Focus on **real behavior**:

* Clicks
* Signups
* Preorders
* Time spent
* Money spent

> ğŸ”¥ **â€œIf they donâ€™t pay attention now, they wonâ€™t pay later.â€**

---

## ğŸ§­ Final Insight: Pretotyping Is an Innovation Mindset

> ğŸ’¡ **â€œPretotyping is not just a testing toolâ€”itâ€™s a cultural shift.â€**

It teaches you to:

* **Think in experiments**
* **Kill ideas early, proudly, and cheaply**
* **Use data, not gut feel**
* **Learn by simulating, not by shipping**

---

# ğŸ§­ **NAVIGATING THE INNOVATION JOURNEY**

---

## ğŸ§— **The Innovatorâ€™s Journey**

> ğŸ¯ **â€œBringing a new idea to life is not a straight pathâ€”itâ€™s a deeply personal adventure filled with uncertainty, resistance, insight, and transformation.â€**

Savoia frames the innovation process as a **modern adaptation of the Heroâ€™s Journey**, a powerful narrative model described by Joseph Campbell. Why? Because innovators, like heroes, must confront **fear, resistance, failure, and self-doubt**â€”not just market challenges.

This metaphor is not just poeticâ€”it provides a **psychological roadmap** that prepares innovators for the ups and downs of product development and internal advocacy.

---

### ğŸ”¥ Stage 1: **The Spark â€” The Call to Adventure**

> ğŸ’¡ **â€œEvery innovation starts with a moment of excitementâ€”a belief that things can be better.â€**

This is when:

* A **frustration with the status quo** triggers a creative impulse.
* A personal insight or customer pain point inspires a concept.
* A founder says, â€œThere *has* to be a better way.â€

âš ï¸ But early excitement is dangerous because:

* You may **fall in love with your solution** before testing the problem.
* You assume others will see what you see.
* You become vulnerable to **confirmation bias**.

> ğŸ§  **â€œBeware of confusing the emotional thrill of a new idea with actual market demand.â€**

---

### ğŸ§ª Stage 2: **The Test â€” Crossing into Reality**

> ğŸ§ª **â€œThe moment you run your first pretotyping experiment, you step into the real world.â€**

Here, the innovator **faces the truth**:

* **Are people interested?**
* **Will they click, sign up, engage, or pay?**
* Or will they **ignore, bounce, or criticize**?

This stage brings resistance:

* From users (lack of interest),
* From team members (who fear change),
* From **yourself** (fear of failure or data that contradicts your vision).

> ğŸ” **â€œThis is the stage where the idea stops being funâ€”and starts being real.â€**

Yet this is where **learning accelerates**, if you have the courage to:

* Kill a weak idea quickly,
* Pivot based on feedback,
* Stay emotionally detached.

> âœ… **â€œStrong innovators fall in love with the *problem*, not the *solution*.â€**

---

### ğŸ” Stage 3: **Validation, Iteration, or Rejection**

> ğŸ“Š **â€œPretotyping shows you whether youâ€™re on the right trackâ€”early, cheaply, and with brutal honesty.â€**

This is the decision point:

* If **ILI (Initial Level of Interest)** is strong,

  * Go deeper,
  * Consider building a prototype or MVP.
* If ILI is weak,

  * Re-test with another pretotyping method.
  * **Pivot** the positioning, market, or features.
* If there's still no traction,

  * Kill it, **without shame**.

#### âœ… Example:

You test an app for time-boxed study sessions with college students.

* Fake Door test yields 500 signups in 3 days.
  â†’ Signal to move forward.

A different feature (group voice rooms) yields 1% clickthrough.
â†’ Kill that feature or pivot the pitch.

> ğŸ’¡ **â€œPretotyping doesnâ€™t just validate ideasâ€”it forces clarity.â€**

---

### ğŸš€ Stage 4: **Scaling or Strategic Exit â€” The Return with the Elixir**

> ğŸ§  **â€œOnce you've proven people want your idea, the challenge shifts from testing to building.â€**

This is the moment to:

* Pitch investors or management with **data, not dreams**.
* Build your MVP with **evidence-backed features**.
* Hire the right team and expand distribution.

But also recognize:

* **Not every idea deserves scaling**.
* Sometimes a great concept is too early, too niche, or not strategically aligned.
* Knowing when to **exit gracefully** is a sign of strategic maturity.

> âœ… **â€œThe goal of the innovatorâ€™s journey is not just to launchâ€”but to discover whatâ€™s worth launching.â€**

---

## ğŸ§± **Organizational Barriers**

> ğŸ§± **â€œEven the best ideas will fail if the culture punishes curiosity and rewards delivery at all costs.â€**

In this chapter, Savoia turns his lens to the **systemic and structural forces** inside companies that make innovation difficultâ€”**even when teams do everything right.**

---

### ğŸ§¨ Barrier 1: **Incentives Reward Building, Not Learning**

> âŒ **â€œIn many companies, building something bad is rewarded more than learning not to build it at all.â€**

Teams are often pressured to:

* Launch on time rather than test first.
* Hit delivery KPIs instead of insight milestones.
* **Look busy**, not **be effective**.

ğŸ§  What this produces:

* Teams ignore weak signals.
* Stakeholders avoid â€œfailureâ€ at all costsâ€”even if it means wasting millions.

#### ğŸ§ª Example:

A team builds a product no one tested.
It flopsâ€”but since it was delivered on time, the project manager is promoted.

> âœ… **â€œWe must start rewarding people for killing bad ideas early.â€**

---

### ğŸ­ Barrier 2: **Corporate Theater and the Illusion of Certainty**

> ğŸ­ **â€œIn the absence of data, companies perform theaterâ€”complete with fake confidence, assumptions, and plans.â€**

Symptoms of **corporate innovation theater**:

* 100-slide business cases based on optimistic projections.
* â€œValidatedâ€ user personas based on guesswork.
* Gantt charts that schedule creativity.

> âŒ **â€œExecutives often ask for certaintyâ€”but real innovation starts with admitting what you donâ€™t know.â€**

---

### ğŸŒ± Solution: **Build a Culture of Experimentation**

> ğŸŒ± **â€œA culture that celebrates learning will always outperform one that punishes failure.â€**

Organizations that innovate well:

* Allocate **small budgets and fast timelines** to idea tests.
* Celebrate **validated pivots** and **early kills**.
* Create **safe-to-fail zones** (e.g., 20% time, hack weeks, innovation sprints).

#### âœ… Real Example: Google X

Google's innovation lab encourages teams to kill their own ideas.

> â€œWe reward people who stop projects, not just those who launch them.â€

> ğŸ“ˆ **â€œInnovation ROI improves drastically when failure is fast, cheap, and instructive.â€**

---

### ğŸ” Reframing Success and Failure

| Traditional Thinking  | The Right It Mindset                  |
| --------------------- | ------------------------------------- |
| Delivering = Winning  | **Validating = Winning**              |
| Failure = Shame       | **Early failure = Smart learning**    |
| Certainty = Strength  | **Admission of ignorance = Strength** |
| Big launch = Progress | **Early traction = Real progress**    |

> ğŸ”¥ **â€œA successful innovation system doesn't prevent failureâ€”it detects and absorbs it early.â€**

---

## âœ… Final Takeaways from Part 4

> ğŸ’¡ **â€œInnovation is not just a technical or market challengeâ€”itâ€™s a personal and cultural one.â€**

### For Innovators:

* Embrace **uncertainty and feedback**.
* Expect emotional highs and lows.
* Be ready to **pivot, kill, or defend** your ideaâ€”with data, not ego.

### For Leaders and Organizations:

* Create **safe spaces to test and fail**.
* Reward **learning and insights**, not just execution.
* Dismantle incentives that promote **building The Wrong It**.

> ğŸ§  **â€œThe Right It isn't found through planningâ€”it's discovered through testing, humility, and bold honesty.â€**

---

# ğŸ§  **BEHAVIORAL & CULTURAL SHIFTS**

---

## ğŸ”„ **Shifting Mindsets**

> âš ï¸ **â€œThe main obstacle to innovation is not technology, tools, or timeâ€”itâ€™s peopleâ€™s habits and mindsets.â€**

Savoia insists: even the best tools like pretotyping wonâ€™t lead to successful innovation **unless thereâ€™s a shift in how people think about risk, failure, and experimentation**. Behavioral inertiaâ€”clinging to old processes or fearing failureâ€”is the **root of slow or false innovation**.

---

### ğŸ’¡ From Execution-First to Experimentation-First

> ğŸ”§ **â€œIn most organizations, execution is kingâ€”experimenters are seen as rebels.â€**

That mindset kills innovation. Instead, we must shift toward:

* **Testing ideas before building them**
* **Investing in uncertainty reduction, not just delivery**
* **Viewing every initiative as a hypothesis, not a certainty**

#### ğŸ” Old Mindset vs. New Mindset:

| Old Execution-First Thinking        | New Experimentation-First Thinking                    |
| ----------------------------------- | ----------------------------------------------------- |
| â€œWeâ€™ve planned it, letâ€™s build it.â€ | **â€œLetâ€™s test interest before we build anything.â€**   |
| â€œJust trust your gut.â€              | **â€œLetâ€™s see what users actually do.â€**               |
| â€œFailure looks bad.â€                | **â€œFailure is a signalâ€”it guides better decisions.â€** |
| â€œPerfect the prototype.â€            | **â€œRun pretotypes first, fast and cheap.â€**           |

> âœ… **â€œDonâ€™t execute before validating. Donâ€™t scale what you havenâ€™t tested.â€**

---

### ğŸ§¨ Celebrate Early Idea Kills

> ğŸ§  **â€œKilling a bad idea early is not failureâ€”itâ€™s high-performance decision-making.â€**

Many organizations and founders have been **trained to fear the optics of failure**, even small ones. But this fear leads to:

* **Zombie projects** that burn money and time,
* **Risk-averse behavior** (avoiding novel ideas),
* **Pressure to justify sunk costs**, even when evidence shows the idea doesnâ€™t work.

Instead, organizations should actively **reward those who run honest experiments and make the call to kill weak ideas**.

> ğŸ”¥ **â€œEarly kills save millions later. They should earn medalsâ€”not reprimands.â€**

#### âœ… Real-World Example:

A team at a large SaaS company pitched a new dashboard module.
Instead of building it, they launched a **Fake Door test** with 2,000 existing users.
Only 17 clicked â€œLearn More.â€
The team killed the projectâ€”saving \$400K in dev costs.
They were **congratulated in a company-wide email**.

> ğŸ§  **â€œEvery killed idea is a step closer to The Right It.â€**

---

### ğŸ’¥ Normalize Micro-Failures to Enable Macro-Success

> ğŸ§ª **â€œInnovation is not about avoiding failureâ€”itâ€™s about failing in ways that are small, fast, cheap, and full of learning.â€**

Teams that fear failure **donâ€™t experiment**, which means they never discover better paths.

By contrast, **innovative teams run micro-experiments constantly**:

* 2 versions of a landing page,
* 3 different positioning messages,
* 5 mini-product concepts in one quarter.

These tiny tests are:

* Low-cost,
* Easy to design,
* **Extremely informative**.

> âœ… **â€œA 48-hour failed test beats a 6-month failed launch every time.â€**

---

### ğŸ”„ Organizational Practices to Shift Culture

> ğŸŒ± **â€œCulture is not what leaders sayâ€”itâ€™s what teams do without asking permission.â€**

To make experimentation and learning part of culture:

#### ğŸ“Œ Practical Shifts:

* âœ… **Make every new initiative start with an XYZ Hypothesis**
* âœ… Give product teams **monthly mini-budgets** just for pretotyping
* âœ… Publicly celebrate **pivot stories and honest kills**
* âœ… Measure **learning velocity**, not just shipping velocity
* âœ… Hold â€œTest-What-Mattersâ€ weeks or Pretotyping Hackathons

#### âœ… Cultural Example:

Spotifyâ€™s team uses a **â€œThink It â€“ Build It â€“ Ship It â€“ Tweak Itâ€** model.
They **donâ€™t ship until â€œThink Itâ€ is validated**â€”and that stage includes **MEH tests and engagement metrics**, not just wireframes.

---

## ğŸŒ± **The Right It in Practice**

> ğŸš€ **â€œThe Right It isnâ€™t a theoryâ€”itâ€™s a proven, scalable system for real-world validation.â€**

This chapter shares **case studies and field-tested patterns** to demonstrate **how pretotyping works across startups, corporations, and solo entrepreneurs**.

---

### âœ… Google AdWords â€“ The Mechanical Turk Pretotype

> ğŸ§  **â€œThe original AdWords was a fake interface connected to a team of humans.â€**

Before building the tech:

* Google let users enter ads into a dummy interface.
* **Behind the scenes, humans manually placed the ads**.
* Result: explosive traction â†’ greenlighted automation build.

> âœ… **â€œProof of concept should precede product development.â€**

---

### âœ… IBM â€“ Enterprise Pretotyping

At IBM, a product team ran a **non-functional dashboard demo** to test internal use of predictive analytics.

* The dashboard looked realâ€”but **data was mocked up manually**.
* They measured **engagement, not opinion**.

Result:

* 70% clickthrough,
* Executive sponsors signed off.

> ğŸ”§ **â€œEven in corporate settings, Mechanical Turk and Fake Door tests save huge budgets.â€**

---

### âœ… Individual Entrepreneurs â€“ Small Bets, Fast Lessons

> ğŸ’¡ **â€œThe Right It is your startup co-founderâ€”even if youâ€™re solo.â€**

Examples:

* A UX designer used **One-Night Stand** method to test a design sprint coaching offer on LinkedIn â†’ sold 3 sessions before building a site.
* A developer created a **Re-label Test** for his focus appâ€”marketing it as â€œDeep Work Tool for Lawyersâ€ â†’ 400% higher conversion.

> ğŸ¯ **â€œPretotyping gives the solo entrepreneur leverageâ€”data replaces doubt.â€**

---

### ğŸ“„ Templates

#### ğŸ§© **XYZ Hypothesis Builder**

> â€œX people will do Y within Z time.â€

Use this to define your test goal:

* 100 developers will click â€œJoin Waitlistâ€ within 3 days.

#### ğŸ§ª **Market Engagement Hypothesis (MEH)**

> â€œIf we offer this, people will respond this wayâ€¦â€

Define the behavior you expect (click, sign up, pay).

#### ğŸ“Š **Initial Level of Interest (ILI) Tracker**

> Formula: **ILI = # who engage / # exposed**

Helps compare multiple experiments and **make data-informed Go/Kill decisions**.

---

### ğŸ§° Tools & Frameworks

| Need                     | Suggested Tools                     |
| ------------------------ | ----------------------------------- |
| **Landing Pages**        | Carrd, Webflow, Unbounce            |
| **Analytics & Heatmaps** | Google Analytics, Hotjar, Crazy Egg |
| **Form Builders**        | Tally, Typeform, Google Forms       |
| **Prototyping**          | Figma, InVision, Marvel             |
| **No-Code MVPs**         | Bubble, Glide, Adalo                |
| **A/B Testing**          | Google Optimize, Optimizely         |

> âœ… **â€œYou donâ€™t need codeâ€”you need curiosity and a clear hypothesis.â€**

---

## ğŸ¯ Final Takeaway: Culture Eats Tools for Breakfast

> ğŸ”¥ **â€œTools donâ€™t drive innovationâ€”mindsets do.â€**

If your team:

* Experiments fast,
* Embraces micro-failures,
* Prioritizes behavioral validation,
* Kills bad ideas early and proudlyâ€¦

Then you are already ahead of 90% of startups and corporate innovation teams.

> ğŸ§  **â€œThe Right It is not just a productâ€”itâ€™s a habit of mind.â€**

---



# Quotes


# References
