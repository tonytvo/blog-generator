---
title: the right IT by Alberto Savoia summary
date: "2025-06-28T22:12:03.284Z"
description: "acquirer multiple by Tobias Carlisle summary"
tags: ["softwaredevelopment", "productmanagement"]
---

# ğŸ§¨ **WHY SO MANY IDEAS FAIL**

---

## ğŸ“‰ **The Law of Market Failure**

### ğŸ”º **â€œMost ideas fail in the marketâ€”even good ones.â€**

Alberto Savoia introduces one of the bookâ€™s cornerstone ideas: the **Law of Market Failure**.

> âœ… **â€œThe Law of Market Failure states that most new ideasâ€”regardless of how promising they seemâ€”will fail in the market.â€**

This is not just anecdotal; **decades of data** across industries show a **consistent 80% to 90% failure rate** for:

* Startups,
* New product launches,
* Marketing campaigns,
* Internal corporate innovations.

### âŒ **Not Technical Failure â€” Market Failure**

Most teams mistakenly believe that if they **build a good product**, customers will naturally come. But:

> **â€œThe market doesnâ€™t care how good your idea is if it doesnâ€™t solve a real problem.â€**

Even the **best engineered, beautifully designed** products fail **if nobody wants them.**

---

### ğŸ§ª **Three Types of Risk in Innovation**

Savoia breaks down innovation risks into 3 types:

1. **Technical Risk** â€“ Can we build it?
2. **Execution Risk** â€“ Can we deliver it on time, on budget?
3. **Market Risk** â€“ **Will they use/buy it?**

Most teams focus obsessively on the first twoâ€”and **neglect the most dangerous one: Market Risk.**

> âœ… **â€œIf the market doesnâ€™t want it, the rest doesnâ€™t matter.â€**

---

### ğŸ­ **The Illusion of Progress**

One of the most damaging traps is the **illusion of progress**. Teams believe they're making headway because they're:

* Hiring engineers,
* Holding design sprints,
* Completing prototypes,
* Writing code,
* Launching MVPsâ€¦

â€¦but theyâ€™re **not testing the core assumption**: **Do people actually want this?**

> ğŸ”¥ **â€œWe confuse activity with progress.â€**

---

### âš ï¸ Real-World Examples:

#### âŒ Google Wave (2009)

* **Massive hype**, elite engineering team, beautiful UI.
* But: **no one understood the problem it solved**.
* **Shut down after 1 year.**

#### âŒ Juicero

* High-tech juicer startup. Raised \$120M.
* Required proprietary juice packs. A **manual squeeze** did the same thing.
* Exposed by journalists. **Collapsed under ridicule.**

#### âœ… Dropbox

* Before building anything, the founders created a **demo video** explaining the product.
* **Measured signups and interest** before investing in infrastructure.
* **Pretotyped**, then built.

---

## ğŸ§  **Mindsets That Lead to Failure**

Savoia argues that idea failure is often **not a business problem, but a psychology problem**.

> âœ… **â€œWe donâ€™t fail because of poor execution. We fail because of self-deception.â€**

---

### 1ï¸âƒ£ **The Reality Distortion Field**

Coined at Apple to describe Steve Jobsâ€™ ability to bend perception, but when used by inexperienced innovators, it leads to disaster.

* **Founders fall in love** with their vision.
* They **filter out negative feedback**, dismiss critics, and overestimate demand.

> â— **â€œWhen you believe in your idea too much, you lose sight of reality.â€**

#### Example:

* A team builds an app to gamify learning Chinese.
* Early feedback: â€œToo gimmicky.â€
* They ignore it, saying users â€œjust need more exposure.â€
* After 6 months, usage drops to near-zero.

---

### 2ï¸âƒ£ **Overconfidence Bias**

> **â€œBecause we came up with the idea, we assume others will love it too.â€**

This bias leads to:

* Ignoring competitive research,
* Skipping market validation,
* Launching too soon.

#### Example:

* A founder says, *â€œIâ€™d use this, so everyone else will too.â€*
* But **you are not your customer.**
* Without evidence, this assumption is **a gamble, not a strategy.**

---

### **Survivorship Bias**

   > **â€œWe focus only on the winners and ignore the graveyard of failed products.â€**
   > Most books, talks, and blog posts feature success stories. We don't hear about the hundreds of failed apps, A/B tests, or product pivots that died quietly.

ğŸ“Œ **Case Study**:
Facebookâ€™s "Stories" format succeeded after copying Snapchat. But Facebook had previously launched and killed many storytelling and ephemeral content features. The public sees only the **winning version**, not the **failed experiments**.

---

### 3ï¸âƒ£ **Confirmation Bias**

* Innovators selectively look for data that **supports** their idea.
* They ignore or rationalize **contradictory evidence**.

> **â€œWe become detectives searching for cluesâ€”only we ignore anything that doesnâ€™t fit our theory.â€**

#### Example:

* 100 people visit your fake landing page.
* 5 people sign up.
* You tell yourself: *â€œLook, 5% interest!â€*
* You ignore: *â€œ95% bouncedâ€”they didnâ€™t care.â€*

---

### 4ï¸âƒ£ **Emotional Attachment to the Idea**

> ğŸ’” **â€œThe more time you spend on an idea, the harder it is to let goâ€”even if itâ€™s wrong.â€**

This is called the **sunk cost fallacy**. Teams:

* Keep tweaking,
* Keep iterating,
* Keep believing...

...instead of **testing core assumptions** or **killing the idea early**.

---

### âœ… **The Better Mindset: Fall in Love With the Problem**

One of the most important quotes in the book:

> ğŸ’¡ **â€œFall in love with the problem, not the solution.â€**

* Stay **obsessed with solving a pain** or fulfilling a need.
* Be **open to changing your solution**â€”or abandoning itâ€”if it doesnâ€™t serve that goal.
* Innovation is **not about being right from the start**, but about **discovering whatâ€™s right** through experimentation.

---

## ğŸ² **3. Our Ideas Are Mostly Guesses â€“ And Thatâ€™s Okay**

### ğŸš§ **All Ideas Are Assumptions Until Proven Otherwise**

> **â€œYour roadmap is a graveyard of guesses.â€**

Gilad emphasizes that **ideation** is inherently **speculative**. Great product thinking isnâ€™t about being right from the startâ€”itâ€™s about being **adaptive and humble**.

---

### ğŸ’¬ **The Expert Myth**

> **â€œEven the most experienced product leaders are wrong most of the time.â€**

Experts have **pattern recognition**, but patterns donâ€™t guarantee correctness. Markets change. Contexts shift. **You need evidence.**

ğŸ“Œ **Example**:
At Google, even senior engineers and PMs often failed A/B tests they were confident in. Over time, this **eroded reliance on opinion** and built a culture of **testing everything**.

---

### ğŸ” **HiPPO Decision-Making: A Red Flag**

> **â€œWhen the Highest Paid Personâ€™s Opinion overrides evidence, the team is flying blind.â€**

To shift to better product thinking:

* Replace **opinions** with **observations**.
* Replace **authority** with **user insight**.
* Replace **roadmaps** with **evidence ladders** (discussed later in the book).

### ğŸ§ª **The Experimentation Backlog**

> **â€œThe product roadmap is not a delivery queueâ€”itâ€™s a hypothesis list.â€**

If most ideas fail, we should not treat them as **projects to build**, but as **guesses to test**.

ğŸ“Œ **Real-World Insight**:
Instead of saying: â€œWe will launch Feature A in Q2,â€ say:
ğŸ” â€œWe believe Feature A may solve Problem X, and we will test it via a prototype or limited release before scaling.â€

---

## ğŸš€ **4. Output â‰  Outcome â€“ The Root of False Productivity**

### âš™ï¸ **Output: Building for the Sake of Delivery**

> **â€œMany teams mistake activity for progress.â€**

Output is:

* Features shipped
* Sprints completed
* Code written

But none of these **guarantee value** to users or business.

---

### ğŸ“ˆ **Outcome: Real, Measurable Impact**

> **â€œOutcome is the real north star: changes in user behavior that create value.â€**

Examples:

* Increase in daily active users (DAU)
* Reduced churn
* Higher customer satisfaction
* Increased conversion rate

ğŸ“Œ **Contrast Example**:

| Metric      | Output                  | Outcome                                     |
| ----------- | ----------------------- | ------------------------------------------- |
| Feature     | Rolled out dark mode    | 40% of users enable it & use app 10% longer |
| Performance | Shipped faster checkout | Conversion rate rises by 15%                |
| Marketing   | Sent newsletter         | Open rates rise; reactivation improves      |

---

### ğŸ­ **Vanity Metrics: Dangerous Illusions**

> **â€œJust because itâ€™s measurable doesnâ€™t mean itâ€™s meaningful.â€**

Examples:

* Number of tickets closed
* Story points completed
* Number of deployments

These **donâ€™t correlate** with user or business value. They can make teams feel good, but **hide real problems**.

---

#### ğŸ” **Shipping â‰  Success**

Gilad warns of the **release trap**â€”celebrating launches while ignoring results.

ğŸ“Œ **Better Practice**:
Measure adoption, usage, behavior change **after launch**. Treat releases as **experiments**, not finish lines.

---

### ğŸ”„ RECAP: Why Most Ideas Fail

| Trap                     | Why It Happens                            | What to Do Instead                                          |
| ------------------------ | ----------------------------------------- | ----------------------------------------------------------- |
| **Assuming demand**      | You think â€œIf I build it, they will comeâ€ | Use **pretotyping** to test demand before building          |
| **Illusion of progress** | You celebrate activity, not results       | Measure **engagement, not effort**                          |
| **Overconfidence**       | You trust your gut too much               | Demand **external evidence**                                |
| **Confirmation bias**    | You only see the good signals             | Track **all user behavior**, not just highlights            |
| **Emotional attachment** | Youâ€™ve invested too much to let go        | Remember: **Killing a bad idea early saves time and money** |
| Ideas are facts                      | Ideas are **guesses** to be tested               | |
| Experts know what works              | Experts also need **validation**                 | |
| Shipping = success                   | **Impact = success**                             | |
| Measure speed and volume (output)    | Measure **value and outcome**                    | |
| Decide based on opinion or authority | Decide based on **evidence and experimentation** | |


---


# âš™ï¸ MAKE SURE YOU HAVE THE RIGHT IT

## ğŸ” **Pretotyping vs. Prototyping**

### ğŸ§  **"Donâ€™t just build it right. First, make sure youâ€™re building the right 'It'."**

One of the **most dangerous myths** in product development is the idea that **if you build a great product, customers will come**. This chapter breaks that myth by contrasting **prototyping** with a more critical, often ignored step: **pretotyping**.

---

### ğŸ§ª **What Is a Pretotype?**

> **"Pretotyping is about testing the marketâ€™s genuine interest in your ideaâ€”before you build anything real or expensive."**

Pretotyping helps you **fail fast and cheaply**, so you donâ€™t **succeed at building something no one wants.**

**Pretotype = Pre + Prototype**
Itâ€™s **not a half-built product**â€”itâ€™s a **simulation or illusion** designed to answer the only question that matters early on:

> â“ **"Will they use it if we build it?"**

---

### ğŸ”§ **What Is a Prototype?**

> **"A prototype answers the question: â€˜Can we build it?â€™"**

Itâ€™s about testing features, functionality, usability, design, etc. It assumes youâ€™ve already validated market interestâ€”which is often **not the case**.

---

### ğŸ“Š Why Pretotyping Must Come First

Savoia insists:

> âœ… **â€œIt is far cheaper to test an ideaâ€™s desirability than to assume it and risk full development.â€**

---

### ğŸš€ Real-World Case Examples:

#### âœ… **Zappos (Shoes Online)**

* Founder **Nick Swinmurn** didnâ€™t build an e-commerce system first.
* He went to local shoe stores, took **photos**, and listed them online.
* When someone ordered, he went and bought the shoes manually.

ğŸ‘‰ **Pretotyping**: Validated **"Would people buy shoes online?"**

---

#### âŒ **Segway**

* \$100M+ invested in development.
* World-class design and tech.
* Assumed it would **revolutionize transportation**.
* Reality: **No one knew where to ride it or wanted to change habits.**

ğŸ‘‰ They **prototyped** brilliantly, but **never pretotyped**.

---

## ğŸ”§ **The Right It Tools & Metrics**

This chapter introduces a **framework of behavioral hypotheses and metrics** to help you know if youâ€™re on the path toward **The Right It**. You donâ€™t have to guessâ€”you test with data.

---

### ğŸ“Œ **1. The XYZ Hypothesis**

> ğŸ“ **"X people will do Y within Z time."**

This is the core of every pretotypeâ€”it forces you to be specific, measurable, and accountable.

---

#### ğŸ” Why Itâ€™s Powerful:

* Prevents **vague hopes** like â€œpeople will love this.â€
* Encourages **clear, testable predictions**.

#### âœ… Example:

> â€œ**500 people will click the â€˜Join Waitlistâ€™ button for our new budgeting app within 5 days**.â€

#### âŒ Bad version:

> â€œPeople will probably be interested in our app.â€ (No numbers, no time frame, no behavior.)

---

### ğŸ§ª **How to Use XYZ Hypotheses Effectively:**

* **X** = How many people?
* **Y** = What action shows interest? (click, sign up, preorderâ€¦)
* **Z** = In what time frame?

> âœ… **â€œIf you canâ€™t test it in time and with numbers, itâ€™s not a real hypothesis.â€**

---

### ğŸ§¬ **2. Market Engagement Hypothesis (MEH)**

> **â€œThe MEH is where your XYZ Hypothesis meets reality.â€**

This is your assumption about how the market will respond when given the **opportunity to act**, even with **no real product yet**.

---

### ğŸ§ª How to Run a MEH Test:

* Use a **landing page** with a fake offer and a CTA (Buy Now, Join Beta).
* Use **ads** to see if people click on a fake product.
* Track real user behavior, not just traffic or likes.

#### âœ… Example:

Create a simple site:

> â€œNew App: StudyTime â€” Beat Procrastination. Sign up for early access.â€
> ğŸ‘‰ Measure signups (Y) over 5 days (Z) from 1,000 visitors (X).

---

### ğŸ“Š **3. Initial Level of Interest (ILI)**

> **"ILI = People who take action / People exposed to the test."**

This is your **conversion rate**, and it's a direct signal of potential market demand.

#### âœ… Example:

* 1,000 visitors.
* 80 clicked "Sign Up".
* ğŸ‘‰ **ILI = 8%**

You can now **compare** this against your target. If your goal was **5%**, then 8% is strong validation.

> ğŸ“ˆ **"ILI transforms qualitative ideas into quantitative traction."**

---

### ğŸ” **Whatâ€™s a Good ILI?**

Savoia doesn't give hard rulesâ€”it varies by marketâ€”but generally:

* **>10%** = promising traction.
* **5â€“10%** = further testing or iteration needed.
* **<5%** = likely weak demand.

> ğŸš« **â€œDonâ€™t chase unicorns with limp engagement metrics.â€**

---

### ğŸ‘©â€ğŸ”¬ **4. High-Expectation Customers (HXC)**

> ğŸ§  **â€œThe best feedback doesnâ€™t come from average usersâ€”it comes from the most demanding ones.â€**

**HXCs** are:

* Already searching for a solution,
* Deeply understand the problem,
* Hard to impress.

But if **you win them**, you're likely building something great.

---

### ğŸ¯ Why Target HXCs Early?

* They provide **blunt, high-signal feedback**.
* If they **adopt** your pretotype, it's a green light.
* If they **ignore** or **criticize** it, you're in danger.

#### âœ… Example: Launching a writing AI tool

* Your HXCs = **daily writers**, **bloggers**, **content creators**.
* Place your fake landing page in **Reddit/r/copywriting** or a writers' forum.
* Monitor engagement and qualitative comments.

> âœ… **â€œIf the people who need it most donâ€™t care, why would the general public?â€**

---

## ğŸ§­ **Mindset and Measurement Shift: The New Way to Innovate**

| Old Mindset            | The Right It Mindset                     |
| ---------------------- | ---------------------------------------- |
| Guess and build        | **Test and measure**                     |
| Listen to opinions     | **Watch actual behavior**                |
| Invest early           | **Validate early, build later**          |
| Seek praise            | **Seek honest disinterest or rejection** |
| Perfect your prototype | **Perfect your XYZ and MEH tests first** |

---

## âœ… Summary: The Right It Toolkit in Practice

| Tool               | Purpose                                 | What It Reveals                               |
| ------------------ | --------------------------------------- | --------------------------------------------- |
| **XYZ Hypothesis** | Define your expected market behavior    | Are you making a testable prediction?         |
| **MEH**            | Run a quick, fake or simulated test     | Will people take action now?                  |
| **ILI**            | Quantify initial traction               | How strong is early interest?                 |
| **HXC**            | Stress-test your idea on early adopters | Is it compelling to the most demanding users? |

> ğŸš€ **â€œIf you canâ€™t get people to engage with a fake version of your product, they probably wonâ€™t care when itâ€™s real.â€**

---

# ğŸ¯ **THE IMPACT-FIRST MINDSET**

*How High-Impact Teams Think, Plan, and Build*

This section reorients teams away from the traditional **output-driven model**â€”where features and velocity dominate thinkingâ€”to a more effective paradigm: **impact-first thinking**, where the **primary goal is to drive measurable improvements** in business and user outcomes through **learning, iteration, and evidence**.

---

## ğŸ”„ **What Are We Optimizing For? â€” Speed or Value?**

### âŒ **The Default (Flawed) Mental Model**

> **â€œBuild more, ship faster, and success will follow.â€**

This model **confuses activity with progress**. Agile teams, CI/CD pipelines, and sprint velocities become the north star.

However, **speed without direction equals waste**.

### âœ… **Reframed Mental Model**

> **â€œProduct success = delivering *value* to users and business through validated learning.â€**

Gilad reframes the question from:

> â€œ**What features are we building this quarter?**â€
> to
> â€œ**What outcomes are we aiming to achieve?**â€

---

### ğŸ“Œ **Analogy: The Compass vs. The Speedometer**

* **Output-first mindset** = checking the speedometer: â€œHow fast are we going?â€
* **Impact-first mindset** = checking the compass: â€œAre we heading in the right direction?â€

> **â€œProgress without direction is just wasted motion.â€**

---

## âš–ï¸ **Defining Impact â€” The Two-Sided Equation**

### ğŸ” **What Is Impact, Really?**

> âœ… **â€œImpact = User Value + Business Valueâ€**

Success only happens when:

* Users **benefit meaningfully**
* The business **gains value** (revenue, retention, efficiency, etc.)

---

### ğŸ§ **User Value**

Value delivered to the end user. It must solve a **real problem** or enable a **meaningful improvement**.

Examples:

* Easier onboarding
* Feature discoverability
* Better UX / accessibility
* Saving user time, effort, or money

> **â€œIf your product isnâ€™t helping users succeed, it wonâ€™t survive.â€**

---

### ğŸ’¼ **Business Value**

Concrete, measurable contributions to business objectives:

* Higher conversion rate
* Improved retention
* Increased revenue per user
* Reduced churn or support costs

> âœ… **â€œTrue product impact connects user success to business success.â€**

ğŸ“Œ **Example**:
A redesigned signup flow that cuts time in half (user value) and improves conversion from 10% â†’ 14% (business value).

---

## ğŸ§° **The GIST Framework â€” Bridging Vision to Action**

Gilad introduces **GIST** as a flexible, scalable tool to support evidence-driven impact delivery:

> **GIST = Goals â†’ Ideas â†’ Step-Projects â†’ Tasks**

It aligns **strategic thinking (Goals)** with **tactical execution (Tasks)** through **validated learning**.

---

### ğŸ¥… **G = Goals**

> âœ… **â€œGoals are clear, measurable impact objectives, not features.â€**

Examples:

* Increase 30-day user retention from 25% to 35%
* Reduce cart abandonment by 15%
* Boost Net Promoter Score (NPS) from 40 to 60

ğŸ“Œ **Goals must be:**

* **Outcome-based** (not output-based)
* **Quantifiable**
* **Time-bound**

---

### ğŸ’¡ **I = Ideas**

> **â€œIdeas are hypothesesâ€”guesses about how to reach a goal.â€**

Most teams confuse ideas with requirements or specs. But in reality:

> â— **â€œIdeas are just untested beliefs, no matter how logical they seem.â€**

Teams must:

* Generate multiple ideas (divergent thinking)
* **Score** them using tools like **ICE** or **RICE**
* Track them in an **Idea Bank** (as covered later in the book)

ğŸ“Œ **Example**:
To increase onboarding completion, ideas might include:

* Reduce signup fields
* Add welcome video
* Enable social login

Each is a guessâ€”not a guarantee.

---

### ğŸ§ª **S = Step-Projects**

> âœ… **â€œFast, inexpensive experiments that test assumptions behind ideas.â€**

This is where **learning happens**.

Examples:

* Fake door tests
* Landing page A/B tests
* Concierge MVPs
* Email split tests

> **â€œDonâ€™t build until you test.â€**

ğŸ“Œ **Case Example**:
Rather than build a complex refer-a-friend system, test user interest with:

* A â€œRefer a Friendâ€ button that logs clicks
* Manual coupon delivery (instead of full automation)

> ğŸ” **â€œStep-projects reduce risk while increasing certainty.â€**

---

### âš™ï¸ **T = Tasks**

> **â€œTasks are delivery items created only after validation.â€**

Once a step-project proves that an idea is **viable and impactful**, it moves into actual development.

> âŒ **â€œDonâ€™t start with tasks. Start with goals and validation.â€**

ğŸ“Œ GIST ensures this **bottom-up execution aligns with top-down strategy**.

---

### ğŸ”„ **GIST in Action: Example Scenario**

**Goal**: Reduce time to first value (TTFV) by 30%
**Ideas**:

* Pre-fill user setup form
* Auto-recommend features based on usage
* Add onboarding video
  **Step Projects**:
* Run A/B test for pre-filled form
* Email new users with video vs. no video
  **Tasks**:
* Develop pre-fill automation
* Design a recommendation engine

Only after **measuring impact** do teams commit engineering time.

---

## ğŸ—“ï¸ **Impact-First Planning â€” Redesigning the Product Planning Process**

### âŒ **Problem with Traditional Roadmaps**

> **â€œMost roadmaps are just a list of guesses with deadlines.â€**

These:

* Create false certainty
* Encourage overcommitment
* Limit adaptability
* Reward shipping, not learning

---

### âœ… **The Impact-First Alternative**

> **â€œStart with outcomes. Let solutions emerge from evidence and experimentation.â€**

Instead of â€œWeâ€™ll build feature X by July,â€ say:

> **â€œWe aim to improve customer activation rate by 20% in Q3.â€**

This keeps:

* **Autonomy** for teams to explore options
* **Alignment** around measurable goals

---

### ğŸ¯ **OKRs Done Right**

Gilad advocates for **real OKRs**, not task lists disguised as goals.

ğŸ“Œ **Correct Format**:

* **Objective**: Improve trial-to-paid conversion
* **KR1**: Increase free trial completion from 50% to 65%
* **KR2**: Raise checkout conversion from 20% to 28%

> âœ… **â€œOKRs should be inspiring, yet grounded in measurable outcomes.â€**

---

## ğŸ§  **Summary â€” Key Mental Shifts for an Impact-First Culture**

| âŒ Output-Driven Thinking     | âœ… Impact-First Thinking                          |
| ---------------------------- | ------------------------------------------------ |
| Deliver more features        | **Deliver more *value***                         |
| Plan by timelines & specs    | **Plan by goals and validated ideas**            |
| Ideas = product requirements | **Ideas = testable hypotheses**                  |
| Success = shipping           | **Success = outcome improvement**                |
| Tasks are the start of work  | **Tasks come last, after goals and experiments** |

---

## âœ… **Final Reflection from Gilad**

> **â€œThe job of a product team is not to deliver featuresâ€”itâ€™s to solve problems and create impact.â€**

This requires:

* A **radical mindset shift**
* New tools (like GIST, OKRs, Confidence Meters)
* A culture of **experimentation, curiosity, and humility**

---



# âš™ï¸ **TESTING YOUR IDEA â€“ THE PRETOTYPE METHOD**

From *The Right It* by **Alberto Savoia**, complete with **bold-highlighted principles**, **deep explanations**, and **real-world, practical examples**.

---

## ğŸ§° OVERVIEW: Why You Need Pretotyping

After understanding **why most ideas fail** (Part 1) and learning **how to define and frame a testable idea** (Part 2), this part focuses on **how to actually test it**â€”quickly, cheaply, and **before building anything real**.

> ğŸ’¡ **â€œA pretotype is not a lesser version of your productâ€”itâ€™s a smarter version of your decision-making process.â€**

Youâ€™ll learn to **simulate usage**, **observe behavior**, and **collect real engagement data** with minimal investment.

---

## ğŸ› ï¸ **The Six Pretotype Methods**

Each pretotype method is a **tactical technique** that allows you to test a specific aspect of user interest or behavior with **minimal effort**.

---

### ğŸ”¹ **1. The Fake Door Test**

> ğŸšª **â€œPut up a door. If no one tries to open it, donâ€™t build the house.â€**

#### âœ… What it is:

You create a **fake entry point** (e.g., a signup button, a â€œBuy Nowâ€ page) for a product or feature that **doesnâ€™t exist yet**.

#### ğŸ¯ Goal:

Measure **user intent** through **clicks, signups, or interest**, without having to build anything.

#### ğŸ§ª How it works:

* Build a **landing page or CTA** for your idea.
* When users click, show a message like:

  > â€œThanks! Weâ€™re not quite ready yet. Join the waitlist and weâ€™ll notify you soon!â€

#### ğŸ§  Example:

A new budgeting app idea â†’

* You run Google Ads with â€œTame Your Spending with BudgetBuddy.â€
* Visitors click â€œTry It Freeâ€ on a basic page.
* You measure clicks, email captures, bounce rate.

> âœ… **If no one clicks the button, why build the product?**

---

### ğŸ”¹ **2. The Infiltrator Test**

> ğŸ•µï¸ **â€œSlip your idea into an existing environment and watch what happens.â€**

#### âœ… What it is:

Insert your product idea **into an existing system**, platform, or experience **without fanfare**â€”and see if people use it naturally.

#### ğŸ¯ Goal:

Validate **real-world fit and organic adoption**.

#### ğŸ§ª How it works:

* Add a small new button, feature, or prompt inside an existing product, store, or page.
* Don't promote itâ€”let it sit.
* Track interactions or neglect.

#### ğŸ§  Example:

You want to test a feature that suggests AI-generated job descriptions.
You quietly embed a â€œTry SmartWriterâ€ button inside your existing HR dashboard.

* Measure how many users discover and click it **organically**.

> ğŸ’¡ **â€œIf users donâ€™t notice or use it when itâ€™s in their flow, it might not matter.â€**

---

### ğŸ”¹ **3. The One-Night Stand Test**

> ğŸŒ™ **â€œOffer your product temporarilyâ€”because permanent commitment is expensive.â€**

#### âœ… What it is:

Provide your product or service for **just one day** (or a limited window) and observe demand and behavior.

#### ğŸ¯ Goal:

Quickly simulate real-world conditions with **short-term exposure** and **no long-term risk**.

#### ğŸ§ª How it works:

* Launch a **popup**, **pop-up store**, **webinar**, or **flash offer**.
* Announce it through minimal ads, email, or social.
* Measure engagement: signups, show-ups, purchases.

#### ğŸ§  Example:

You want to launch a fitness accountability program.

* Offer a **1-day trial event** for \$5 with a live coach on Zoom.
* Track how many sign up, how many show up, how they react.

> ğŸ” **â€œA one-night stand reveals if thereâ€™s chemistryâ€”before you commit to marriage.â€**

---

### ğŸ”¹ **4. The Pinocchio Test**

> ğŸªµ **â€œBuild a fake product that looks realâ€”but doesnâ€™t function.â€**

#### âœ… What it is:

You create a **non-functional mockup** of your product to **observe user interaction** and test comprehension or desirability.

#### ğŸ¯ Goal:

Find out if people **understand and engage** with your product concept, even if it doesnâ€™t work.

#### ğŸ§ª How it works:

* Create a fake version: a dummy app screen, clickable prototype, 3D printed device shell, etc.
* Give it to users.
* Ask them to â€œuse itâ€ as if it were real.

#### ğŸ§  Example:

Testing an â€œAI-powered grocery list planner.â€

* Create a Figma prototype of the app.
* Watch users try to interact.
* Do they know what it does? Where they click first? Do they seem excited or bored?

> âœ… **â€œIf they donâ€™t want the fake version, they wonâ€™t want the real one either.â€**

---

### ğŸ”¹ **5. The Mechanical Turk**

> ğŸ§  **â€œSimulate the output of a machineâ€”using human effort behind the scenes.â€**

#### âœ… What it is:

You **fake a software feature or hardware automation** by **doing the task manually**, while making it look automated.

#### ğŸ¯ Goal:

Test if people will use the functionality **before building complex tech**.

#### ğŸ§ª How it works:

* You create a front-end or interface that mimics the full product.
* Behind the scenes, your team fulfills requests by hand.

#### ğŸ§  Example:

You want to build â€œAI Resume Booster.â€

* Users upload resumes and get edits back.
* Instead of AI, your editor does the work manually.
* You test demand **before building the AI model.**

> ğŸ’¡ **â€œWhy code an algorithm until you know theyâ€™ll pay for the outcome?â€**

---

### ğŸ”¹ **6. The Re-label Test**

> ğŸ” **â€œTake an old product. Give it a new name or use case. See what happens.â€**

#### âœ… What it is:

You test new positioning or audience segments by **repackaging an existing product**.

#### ğŸ¯ Goal:

Test if a **new use case, niche, or branding** resonates better.

#### ğŸ§ª How it works:

* Use your current product or someone elseâ€™s.
* Market it to a new group, with new messaging.
* Track conversion and interest.

#### ğŸ§  Example:

You already sell sleep headphones to travelers.
Re-label them:

> â€œ**Deep Focus Headphones for Remote Developers**â€
> Run new ads. See which segment responds better.

> ğŸ“ˆ **â€œSometimes the product isnâ€™t wrongâ€”the audience is.â€**

---

## ğŸ“ **Innovation Metrics & Decision Frameworks**

> ğŸ“Š **â€œPretotyping without measurement is just theater.â€**

After you run your pretotyping tests, how do you know if your idea is â€œThe Right Itâ€? You need a **decision system based on behavior, not bias**.

---

### ğŸ§  Key Innovation Metrics

#### ğŸ”¸ **XYZ Hypothesis**

> **â€œX people will do Y within Z time.â€**

This turns vague ideas into specific predictions.

#### ğŸ”¸ **MEH â€“ Market Engagement Hypothesis**

> **â€œIf we offer this, people will do that.â€**
> The bridge between concept and test.

#### ğŸ”¸ **ILI â€“ Initial Level of Interest**

> **ILI = (# who take action) / (Total exposed)**

A clean metric to **quantify real engagement**.

---

### ğŸ” What to Do With the Data

| Outcome            | Interpretation        | Decision            |
| ------------------ | --------------------- | ------------------- |
| High ILI (10â€“15%+) | Strong early interest | âœ… Go ahead with MVP |
| Medium ILI (5â€“10%) | Mixed signals         | ğŸ” Iterate or retest |
| Low ILI (<5%)      | Weak or no demand     | âŒ Kill the idea     |

---

### ğŸ’¥ Focus on Behavior, Not Applause

> âŒ Donâ€™t fall for **vanity metrics** like:

* Pageviews
* Likes
* Comments
* Shares

> âœ… Focus on **real behavior**:

* Clicks
* Signups
* Preorders
* Time spent
* Money spent

> ğŸ”¥ **â€œIf they donâ€™t pay attention now, they wonâ€™t pay later.â€**

---

## ğŸ§­ Final Insight: Pretotyping Is an Innovation Mindset

> ğŸ’¡ **â€œPretotyping is not just a testing toolâ€”itâ€™s a cultural shift.â€**

It teaches you to:

* **Think in experiments**
* **Kill ideas early, proudly, and cheaply**
* **Use data, not gut feel**
* **Learn by simulating, not by shipping**

---

# ğŸ”¬ **EVIDENCE-GUIDED DEVELOPMENT**

*â€œSuccess is not about being right from the startâ€”it's about reducing uncertainty smartly.â€*

In this pivotal section, Gilad lays out the practical mechanics for moving from **guesswork and gut-feel** to a system of **learning, experimentation, and evidence-based decision making**. Itâ€™s a shift from **certainty-seeking** to **curiosity-driven building**, supported by data and behavioral insight.

---

### ğŸªœ **The Evidence Ladder â€” A Tool for Judging Idea Quality**

#### ğŸ¯ **The Core Idea**

> âœ… **â€œNot all ideas are created equal. Their strength lies in the *evidence* supporting them.â€**

Most teams donâ€™t evaluate the *quality* of ideasâ€”they prioritize by influence, intuition, or trends. The **Evidence Ladder** gives you a clear framework to rank ideas based on the **reliability of the evidence backing them**.

---

#### ğŸ“¶ **The 5 Levels of the Evidence Ladder**

1. **Speculation**

   > ğŸ§  â€œI just have a feeling this might work.â€
   > These are **pure guesses**, unbacked by any validation.
   > âš ï¸ Risk: **Building from here without testing leads to waste.**

2. **Opinions**

   > ğŸ’¬ â€œThe VP of Sales thinks we need this.â€
   > This includes feedback from stakeholders, teammates, even users. But it's **subjective and biased**.
   > âš ï¸ Still **weak evidence** until verified through action.

3. **User Feedback**

   > ğŸ—£ï¸ â€œUsers told us they want this feature in interviews.â€
   > Valuable, but still **what people *say*, not what they *do***. Needs to be tested behaviorally.
   > âœ”ï¸ Better than speculation, but not sufficient on its own.

4. **User Behavior**

   > ğŸ“Š â€œUsers clicked the fake door button at a 15% rate.â€
   > **Behavioral evidence** (from analytics, A/B tests, click maps) shows what people **actually do**.
   > âœ… **Strong indicator** that the idea creates real engagement.

5. **Business Results**

   > ğŸ’° â€œThe idea increased conversions by 12% and improved LTV.â€
   > The **gold standard**. When a tested idea leads to measurable **business outcomes**, confidence is at its highest.
   > âœ…âœ… High-value ideas live here.

---

#### ğŸ§  **How to Use the Ladder**

> âœ… â€œThe higher an idea sits, the more confidently you can pursue it.â€

ğŸ“Œ Use this as a **prioritization filter**:

* Low-evidence ideas â†’ run **small experiments**
* High-evidence ideas â†’ invest further, scale up
* No-evidence ideas â†’ donâ€™t put on your roadmap yet

---

### ğŸ§ª **Testing Ideas with Step Projects â€” Small Bets, Fast Learning**

#### â— The Problem with Big-Bang Development

> âŒ â€œLetâ€™s build the whole feature, then launch and see if it works.â€

This approach:

* Consumes months of dev time
* Delays feedback
* Increases risk and emotional attachment

---

#### âœ… **Step Projects: The Safer, Smarter Alternative**

> âœ… **â€œStep Projects are mini-experiments designed to test the most critical assumptions of your idea.â€**

They are:

* **Quick** (days to weeks)
* **Cheap** (light engineering or no-code)
* **Focused** (test *one core assumption*)

---

#### ğŸ“Œ **Examples of Step Projects**

| Type                  | Description                                    | Example                                                 |
| --------------------- | ---------------------------------------------- | ------------------------------------------------------- |
| **Fake Door Test**    | Show an option that doesnâ€™t exist yet          | Add â€œUpgrade to Proâ€ button, log clicks                 |
| **Landing Page Test** | A/B test marketing messages or features        | Test new pricing tiers on mock pages                    |
| **Wizard of Oz MVP**  | Fake the backend, simulate experience          | Manual fulfillment of orders to test demand             |
| **Email Split Test**  | Measure response to different concepts or CTAs | Test â€œRefer a Friendâ€ vs â€œGet a Bonusâ€                  |
| **Usability Test**    | Use Figma/Sketch to simulate flows             | Observe users interact with new checkout flow prototype |

---

#### ğŸ” **Step Projects = Fast Learning, Not Fast Building**

> ğŸ§  â€œIf your idea is wrong, better to find out in 2 days than in 3 months.â€

**Key Benefits**:

* Protect team bandwidth
* Learn from **real behavior**
* Build internal culture of *experimentation over certainty*

---

#### ğŸ” **What Makes a Good Step Project?**

* Tests the **riskiest assumption** first
* Has a **clear success metric**
* Is **time-boxed and simple**
* Can be run with minimal disruption

> âœ… â€œDonâ€™t ask â€˜Can we build it?â€™ Ask, *â€˜Should* we build it?â€™â€

---

### ğŸ“ˆ **Learning from Data â€” Build the Learning Engine**

#### ğŸ” **Why Learning Is More Valuable Than Shipping**

> ğŸš« â€œShipping velocity is a vanity metric unless tied to outcomes.â€
> âœ… â€œ**Learning velocity** is the new superpower of product teams.â€

---

#### ğŸ“Š **Sources of Learning Signals**

1. **Quantitative Data**

   * Funnel analytics (e.g., conversion, retention)
   * A/B testing results
   * Heatmaps and session replays
   * Cohort analysis

2. **Qualitative Data**

   * Customer interviews
   * Usability sessions
   * Customer support tickets

---

#### ğŸ“Œ **Example Loop in Action**

**Goal**: Improve onboarding completion
**Idea**: Add progress bar
**Step Project**: A/B test with 20% of new users
**Result**: 18% lift in completion
**Next**: Scale feature, monitor long-term impact

> âœ… â€œTeams should run **dozens of these loops** per quarterâ€”not 1 big risky bet.â€

---

#### ğŸ§  **Key Insight**

> **â€œYour product is not the final outputâ€”*your learning is*.â€**
> Shipping without learning is just output. Learning leads to *outcome*.

---

### ğŸ—‚ï¸ **Prioritizing by Evidence and Value â€” Choose Wisely**

#### âŒ The Reality of Idea Backlogs

> â— â€œMost idea lists are long, unstructured, and driven by emotion.â€

Stakeholders push pet features, trends drive FOMO, and teams build based on **who shouts the loudest**.

---

#### âœ… **Replace Gut Feel with Scoring Models**

##### ğŸ”¢ **ICE Scoring**

> **Impact Ã— Confidence Ã· Effort**

Simple, fast model for quick idea comparisons.

ğŸ“Œ **Example**:

| Idea             | Impact | Confidence | Effort | ICE Score |
| ---------------- | ------ | ---------- | ------ | --------- |
| Simplified login | 8      | 7          | 3      | **18.7**  |
| AI chatbot       | 6      | 3          | 5      | **3.6**   |

---

##### ğŸ“¶ **Confidence Meter**

> âœ… â€œVisual tool that tracks the strength of supporting evidence for each idea.â€

Use color-coded tiers:

* ğŸ”´ Speculative / Opinion
* ğŸŸ¡ Some user feedback
* ğŸŸ¢ Behavior-tested / Result-backed

Helpful for:

* Roadmap debates
* Stakeholder discussions
* Justifying prioritization

---

##### ğŸ§  **RICE Model**

> **Reach Ã— Impact Ã— Confidence Ã· Effort**

Adds scale to the ICE modelâ€”especially for B2C or large platforms.

ğŸ“Œ **Example**:

| Idea             | Reach | Impact | Confidence | Effort | RICE Score |
| ---------------- | ----- | ------ | ---------- | ------ | ---------- |
| Auto-suggestions | 5,000 | 6      | 7          | 4      | 52,500     |
| New dashboard    | 500   | 8      | 8          | 5      | 6,400      |

> âœ… â€œRICE prevents small-impact projects from crowding out high-leverage ones.â€

---

#### ğŸ§  **Giladâ€™s Rule**

> âœ… â€œIdeas with low confidence should get *tiny tests*, not massive investments.â€

This saves:

* Time
* Developer energy
* Morale (you fail fast, not late)

---

## ğŸ§  **RECAP â€” FROM GUESSING TO EVIDENCE-LED BUILDING**

| âŒ Traditional Thinking             | âœ… Evidence-Guided Practice                        |
| ---------------------------------- | ------------------------------------------------- |
| Opinions and authority guide ideas | **Evidence and behavior validate ideas**          |
| Roadmaps filled with assumptions   | **Roadmaps filtered by confidence & testing**     |
| Big launches, slow feedback        | **Small tests, fast learning cycles**             |
| Prioritize by gut or trends        | **Prioritize by ICE, RICE, and Confidence Meter** |
| Learning is accidental             | **Learning is systematic and fast**               |

---

> ğŸ’¬ **â€œThe best teams donâ€™t guess betterâ€”they test better.â€**

This approach is not about being risk-averseâ€”itâ€™s about **taking smarter risks**, fast.

---


# ğŸ§­ **NAVIGATING THE INNOVATION JOURNEY**

---

## ğŸ§— **The Innovatorâ€™s Journey**

> ğŸ¯ **â€œBringing a new idea to life is not a straight pathâ€”itâ€™s a deeply personal adventure filled with uncertainty, resistance, insight, and transformation.â€**

Savoia frames the innovation process as a **modern adaptation of the Heroâ€™s Journey**, a powerful narrative model described by Joseph Campbell. Why? Because innovators, like heroes, must confront **fear, resistance, failure, and self-doubt**â€”not just market challenges.

This metaphor is not just poeticâ€”it provides a **psychological roadmap** that prepares innovators for the ups and downs of product development and internal advocacy.

---

### ğŸ”¥ Stage 1: **The Spark â€” The Call to Adventure**

> ğŸ’¡ **â€œEvery innovation starts with a moment of excitementâ€”a belief that things can be better.â€**

This is when:

* A **frustration with the status quo** triggers a creative impulse.
* A personal insight or customer pain point inspires a concept.
* A founder says, â€œThere *has* to be a better way.â€

âš ï¸ But early excitement is dangerous because:

* You may **fall in love with your solution** before testing the problem.
* You assume others will see what you see.
* You become vulnerable to **confirmation bias**.

> ğŸ§  **â€œBeware of confusing the emotional thrill of a new idea with actual market demand.â€**

---

### ğŸ§ª Stage 2: **The Test â€” Crossing into Reality**

> ğŸ§ª **â€œThe moment you run your first pretotyping experiment, you step into the real world.â€**

Here, the innovator **faces the truth**:

* **Are people interested?**
* **Will they click, sign up, engage, or pay?**
* Or will they **ignore, bounce, or criticize**?

This stage brings resistance:

* From users (lack of interest),
* From team members (who fear change),
* From **yourself** (fear of failure or data that contradicts your vision).

> ğŸ” **â€œThis is the stage where the idea stops being funâ€”and starts being real.â€**

Yet this is where **learning accelerates**, if you have the courage to:

* Kill a weak idea quickly,
* Pivot based on feedback,
* Stay emotionally detached.

> âœ… **â€œStrong innovators fall in love with the *problem*, not the *solution*.â€**

---

### ğŸ” Stage 3: **Validation, Iteration, or Rejection**

> ğŸ“Š **â€œPretotyping shows you whether youâ€™re on the right trackâ€”early, cheaply, and with brutal honesty.â€**

This is the decision point:

* If **ILI (Initial Level of Interest)** is strong,

  * Go deeper,
  * Consider building a prototype or MVP.
* If ILI is weak,

  * Re-test with another pretotyping method.
  * **Pivot** the positioning, market, or features.
* If there's still no traction,

  * Kill it, **without shame**.

#### âœ… Example:

You test an app for time-boxed study sessions with college students.

* Fake Door test yields 500 signups in 3 days.
  â†’ Signal to move forward.

A different feature (group voice rooms) yields 1% clickthrough.
â†’ Kill that feature or pivot the pitch.

> ğŸ’¡ **â€œPretotyping doesnâ€™t just validate ideasâ€”it forces clarity.â€**

---

### ğŸš€ Stage 4: **Scaling or Strategic Exit â€” The Return with the Elixir**

> ğŸ§  **â€œOnce you've proven people want your idea, the challenge shifts from testing to building.â€**

This is the moment to:

* Pitch investors or management with **data, not dreams**.
* Build your MVP with **evidence-backed features**.
* Hire the right team and expand distribution.

But also recognize:

* **Not every idea deserves scaling**.
* Sometimes a great concept is too early, too niche, or not strategically aligned.
* Knowing when to **exit gracefully** is a sign of strategic maturity.

> âœ… **â€œThe goal of the innovatorâ€™s journey is not just to launchâ€”but to discover whatâ€™s worth launching.â€**

---

## ğŸ§± **Organizational Barriers**

> ğŸ§± **â€œEven the best ideas will fail if the culture punishes curiosity and rewards delivery at all costs.â€**

In this chapter, Savoia turns his lens to the **systemic and structural forces** inside companies that make innovation difficultâ€”**even when teams do everything right.**

---

### ğŸ§¨ Barrier 1: **Incentives Reward Building, Not Learning**

> âŒ **â€œIn many companies, building something bad is rewarded more than learning not to build it at all.â€**

Teams are often pressured to:

* Launch on time rather than test first.
* Hit delivery KPIs instead of insight milestones.
* **Look busy**, not **be effective**.

ğŸ§  What this produces:

* Teams ignore weak signals.
* Stakeholders avoid â€œfailureâ€ at all costsâ€”even if it means wasting millions.

#### ğŸ§ª Example:

A team builds a product no one tested.
It flopsâ€”but since it was delivered on time, the project manager is promoted.

> âœ… **â€œWe must start rewarding people for killing bad ideas early.â€**

---

### ğŸ­ Barrier 2: **Corporate Theater and the Illusion of Certainty**

> ğŸ­ **â€œIn the absence of data, companies perform theaterâ€”complete with fake confidence, assumptions, and plans.â€**

Symptoms of **corporate innovation theater**:

* 100-slide business cases based on optimistic projections.
* â€œValidatedâ€ user personas based on guesswork.
* Gantt charts that schedule creativity.

> âŒ **â€œExecutives often ask for certaintyâ€”but real innovation starts with admitting what you donâ€™t know.â€**

---

### ğŸŒ± Solution: **Build a Culture of Experimentation**

> ğŸŒ± **â€œA culture that celebrates learning will always outperform one that punishes failure.â€**

Organizations that innovate well:

* Allocate **small budgets and fast timelines** to idea tests.
* Celebrate **validated pivots** and **early kills**.
* Create **safe-to-fail zones** (e.g., 20% time, hack weeks, innovation sprints).

#### âœ… Real Example: Google X

Google's innovation lab encourages teams to kill their own ideas.

> â€œWe reward people who stop projects, not just those who launch them.â€

> ğŸ“ˆ **â€œInnovation ROI improves drastically when failure is fast, cheap, and instructive.â€**

---

### ğŸ” Reframing Success and Failure

| Traditional Thinking  | The Right It Mindset                  |
| --------------------- | ------------------------------------- |
| Delivering = Winning  | **Validating = Winning**              |
| Failure = Shame       | **Early failure = Smart learning**    |
| Certainty = Strength  | **Admission of ignorance = Strength** |
| Big launch = Progress | **Early traction = Real progress**    |

> ğŸ”¥ **â€œA successful innovation system doesn't prevent failureâ€”it detects and absorbs it early.â€**

---

## âœ… Final Takeaways

> ğŸ’¡ **â€œInnovation is not just a technical or market challengeâ€”itâ€™s a personal and cultural one.â€**

### For Innovators:

* Embrace **uncertainty and feedback**.
* Expect emotional highs and lows.
* Be ready to **pivot, kill, or defend** your ideaâ€”with data, not ego.

### For Leaders and Organizations:

* Create **safe spaces to test and fail**.
* Reward **learning and insights**, not just execution.
* Dismantle incentives that promote **building The Wrong It**.

> ğŸ§  **â€œThe Right It isn't found through planningâ€”it's discovered through testing, humility, and bold honesty.â€**

---

# ğŸ§  **BEHAVIORAL & CULTURAL SHIFTS**

---

## ğŸ”„ **Shifting Mindsets**

> âš ï¸ **â€œThe main obstacle to innovation is not technology, tools, or timeâ€”itâ€™s peopleâ€™s habits and mindsets.â€**

Savoia insists: even the best tools like pretotyping wonâ€™t lead to successful innovation **unless thereâ€™s a shift in how people think about risk, failure, and experimentation**. Behavioral inertiaâ€”clinging to old processes or fearing failureâ€”is the **root of slow or false innovation**.

---

### ğŸ’¡ From Execution-First to Experimentation-First

> ğŸ”§ **â€œIn most organizations, execution is kingâ€”experimenters are seen as rebels.â€**

That mindset kills innovation. Instead, we must shift toward:

* **Testing ideas before building them**
* **Investing in uncertainty reduction, not just delivery**
* **Viewing every initiative as a hypothesis, not a certainty**

#### ğŸ” Old Mindset vs. New Mindset:

| Old Execution-First Thinking        | New Experimentation-First Thinking                    |
| ----------------------------------- | ----------------------------------------------------- |
| â€œWeâ€™ve planned it, letâ€™s build it.â€ | **â€œLetâ€™s test interest before we build anything.â€**   |
| â€œJust trust your gut.â€              | **â€œLetâ€™s see what users actually do.â€**               |
| â€œFailure looks bad.â€                | **â€œFailure is a signalâ€”it guides better decisions.â€** |
| â€œPerfect the prototype.â€            | **â€œRun pretotypes first, fast and cheap.â€**           |

> âœ… **â€œDonâ€™t execute before validating. Donâ€™t scale what you havenâ€™t tested.â€**

---

### ğŸ§¨ Celebrate Early Idea Kills

> ğŸ§  **â€œKilling a bad idea early is not failureâ€”itâ€™s high-performance decision-making.â€**

Many organizations and founders have been **trained to fear the optics of failure**, even small ones. But this fear leads to:

* **Zombie projects** that burn money and time,
* **Risk-averse behavior** (avoiding novel ideas),
* **Pressure to justify sunk costs**, even when evidence shows the idea doesnâ€™t work.

Instead, organizations should actively **reward those who run honest experiments and make the call to kill weak ideas**.

> ğŸ”¥ **â€œEarly kills save millions later. They should earn medalsâ€”not reprimands.â€**

#### âœ… Real-World Example:

A team at a large SaaS company pitched a new dashboard module.
Instead of building it, they launched a **Fake Door test** with 2,000 existing users.
Only 17 clicked â€œLearn More.â€
The team killed the projectâ€”saving \$400K in dev costs.
They were **congratulated in a company-wide email**.

> ğŸ§  **â€œEvery killed idea is a step closer to The Right It.â€**

---

### ğŸ’¥ Normalize Micro-Failures to Enable Macro-Success

> ğŸ§ª **â€œInnovation is not about avoiding failureâ€”itâ€™s about failing in ways that are small, fast, cheap, and full of learning.â€**

Teams that fear failure **donâ€™t experiment**, which means they never discover better paths.

By contrast, **innovative teams run micro-experiments constantly**:

* 2 versions of a landing page,
* 3 different positioning messages,
* 5 mini-product concepts in one quarter.

These tiny tests are:

* Low-cost,
* Easy to design,
* **Extremely informative**.

> âœ… **â€œA 48-hour failed test beats a 6-month failed launch every time.â€**

---

### ğŸ”„ Organizational Practices to Shift Culture

> ğŸŒ± **â€œCulture is not what leaders sayâ€”itâ€™s what teams do without asking permission.â€**

To make experimentation and learning part of culture:

#### ğŸ“Œ Practical Shifts:

* âœ… **Make every new initiative start with an XYZ Hypothesis**
* âœ… Give product teams **monthly mini-budgets** just for pretotyping
* âœ… Publicly celebrate **pivot stories and honest kills**
* âœ… Measure **learning velocity**, not just shipping velocity
* âœ… Hold â€œTest-What-Mattersâ€ weeks or Pretotyping Hackathons

#### âœ… Cultural Example:

Spotifyâ€™s team uses a **â€œThink It â€“ Build It â€“ Ship It â€“ Tweak Itâ€** model.
They **donâ€™t ship until â€œThink Itâ€ is validated**â€”and that stage includes **MEH tests and engagement metrics**, not just wireframes.

---

## ğŸŒ± **The Right It in Practice**

> ğŸš€ **â€œThe Right It isnâ€™t a theoryâ€”itâ€™s a proven, scalable system for real-world validation.â€**

This chapter shares **case studies and field-tested patterns** to demonstrate **how pretotyping works across startups, corporations, and solo entrepreneurs**.

---

### âœ… Google AdWords â€“ The Mechanical Turk Pretotype

> ğŸ§  **â€œThe original AdWords was a fake interface connected to a team of humans.â€**

Before building the tech:

* Google let users enter ads into a dummy interface.
* **Behind the scenes, humans manually placed the ads**.
* Result: explosive traction â†’ greenlighted automation build.

> âœ… **â€œProof of concept should precede product development.â€**

---

### âœ… IBM â€“ Enterprise Pretotyping

At IBM, a product team ran a **non-functional dashboard demo** to test internal use of predictive analytics.

* The dashboard looked realâ€”but **data was mocked up manually**.
* They measured **engagement, not opinion**.

Result:

* 70% clickthrough,
* Executive sponsors signed off.

> ğŸ”§ **â€œEven in corporate settings, Mechanical Turk and Fake Door tests save huge budgets.â€**

---

### âœ… Individual Entrepreneurs â€“ Small Bets, Fast Lessons

> ğŸ’¡ **â€œThe Right It is your startup co-founderâ€”even if youâ€™re solo.â€**

Examples:

* A UX designer used **One-Night Stand** method to test a design sprint coaching offer on LinkedIn â†’ sold 3 sessions before building a site.
* A developer created a **Re-label Test** for his focus appâ€”marketing it as â€œDeep Work Tool for Lawyersâ€ â†’ 400% higher conversion.

> ğŸ¯ **â€œPretotyping gives the solo entrepreneur leverageâ€”data replaces doubt.â€**

---

## ğŸ” **Culture of Evidence-Guided Product Development**

### âŒ **From Authority-Based to Evidence-Based Decisions**

> â— â€œMany product decisions are still driven by the loudest voice in the room.â€

**HiPPOs (Highest Paid Personâ€™s Opinions)**, office politics, and **legacy thinking** often dominate. Gilad urges a **cultural transformation**:

> âœ… **â€œGood ideas can come from anywhereâ€”but only evidence can validate them.â€**

---

### ğŸ” **Build a Culture of Curiosity and Experiments**

> âœ… â€œReplace â€˜proving youâ€™re rightâ€™ with â€˜finding whatâ€™s true.â€™â€

**Key behaviors of a strong evidence-guided culture:**

* **Welcoming failure** as part of learning
* **Celebrating invalidated ideas** for saving resources
* **Rewarding experiments**, not just feature launches

ğŸ“Œ **Example**: A product manager proposes an onboarding chatbot, runs a test, and finds **engagement drops**. Instead of being blamed, the team is praised for **invalidating the wrong bet early.**

---

### ğŸ”„ **Cultural Slogans to Reinforce Mindset**

* â€œ**Test before you build**.â€
* â€œ**Ideas are hypotheses, not promises.**â€
* â€œ**Celebrate small failures that prevent big ones.**â€

> âœ… **â€œCulture eats strategy for breakfastâ€”so make experimentation part of your companyâ€™s identity.â€**

---

## ğŸ‘¥ **Empowered Teams â€” Autonomy with Alignment**

### ğŸ§± **What Empowerment Actually Means**

> âœ… â€œEmpowered teams donâ€™t just executeâ€”they solve problems.â€

Too often, teams are handed a **roadmap of outputs** and told to **deliver on schedule**. This isnâ€™t empowermentâ€”itâ€™s execution under constraint.

---

### ğŸ’¡ **Three Pillars of Empowered Teams**

1. âœ… **Clear Goals**

   * Set by leadership through **OKRs, North Star Metrics, or GIST Goals**
   * Must focus on **outcomes**, not tasks

2. âœ… **Autonomy**

   * Teams choose **how to solve problems**
   * Encourages **ownership**, creativity, and motivation

3. âœ… **Access to Users and Data**

   * Teams **talk to customers directly**
   * Use analytics, A/B testing, and behavior data to learn fast

> âœ… â€œGive teams the *why* and *what success looks like*. Let them figure out *how*.â€

---

### âŒ **Avoid Command-and-Control Cultures**

> â— â€œTelling teams what to build makes them disengaged, less creative, and less accountable.â€

ğŸ“Œ Instead, create **â€œcontext, not control.â€**

**Example**:
Team A is told to â€œbuild feature Xâ€ by Q3.
Team B is asked to â€œincrease user retention by 15%â€â€”and allowed to find the best ideas.
âœ… Team B will **test**, **learn**, and likely **outperform** over time.

---

## ğŸ‘‘ **Leadership for Impact â€” From Director to Enabler**

### ğŸ”„ **Redefine Leadership Roles**

> âœ… â€œLeaders should be *impact coaches*, not taskmasters.â€

Traditional leadership = assigning features, approving specs, demanding timelines
Impact-first leadership = **setting goals**, **enabling learning**, **clearing obstacles**

---

### ğŸ“Œ **What Impact-Oriented Leaders Do**

* **Set the Vision**: Provide long-term direction aligned with company strategy.
* **Define Outcome Goals**: Establish success in terms of **user and business value**, not features.
* **Foster Safety for Experiments**: Normalize failure and learning.
* **Coach, Donâ€™t Command**: Help teams grow autonomy and decision-making capacity.
* **Resource Learning Loops**: Invest in UX research, data infrastructure, experimentation tools.

> âœ… â€œLeadership is about designing the environment where good decisions can emerge.â€

---

### âŒ **Anti-Patterns to Watch For**

* Rewarding delivery over learning
* Killing ideas based on opinion, not data
* Penalizing teams for failed experiments
* Insisting on waterfall-style roadmaps

> â— â€œIf leaders donâ€™t model evidence-based thinking, teams wonâ€™t either.â€

---

## ğŸ§© **Bringing It All Together â€” From Framework to Operating System**

### ğŸ”„ **Combine GIST with Lean, Agile, and Discovery**

GIST is not a replacement for Agile or Leanâ€”itâ€™s an **overlay** that ensures **alignment from strategy to delivery**.

> âœ… â€œGIST makes Agile actually outcome-oriented, not just fast.â€

---

### ğŸ“Œ **How It Integrates**

| GIST Element      | Matches With             | Purpose                       |
| ----------------- | ------------------------ | ----------------------------- |
| **Goals**         | OKRs / North Star Metric | Strategic focus               |
| **Ideas**         | Discovery / Ideation     | Exploration of possible paths |
| **Step Projects** | MVPs / Experiments       | Learn fast, test risk         |
| **Tasks**         | Agile Sprints / Backlog  | Tactical execution            |

---

### ğŸ“ˆ **Shift from Launches to Impact Delivery**

> âŒ â€œLaunch is not the endâ€”itâ€™s the *start* of learning.â€
> âœ… â€œGreat teams donâ€™t just ship featuresâ€”they ship *results*.â€

**Continuous Impact Delivery** means:

* Every sprint is tied to an **impact metric**
* Every launch includes **a feedback loop**
* Roadmaps evolve based on **whatâ€™s working, not what was promised**

> âœ… â€œDonâ€™t plan for certaintyâ€”plan for discovery and adaptability.â€

---

## ğŸ§  **SUMMARY â€” BUILDING THE ENVIRONMENT FOR IMPACT**

| âŒ Traditional Org Model     | âœ… Evidence-Guided Culture & Structure                   |
| --------------------------- | ------------------------------------------------------- |
| Top-down feature mandates   | **Autonomous teams solving outcome goals**              |
| Leaders assign work         | **Leaders coach, define vision, and unblock**           |
| Success = shipping features | **Success = delivering measurable user/business value** |
| Failure is punished         | **Failure is normalized and leveraged for learning**    |
| Roadmaps based on opinions  | **Roadmaps shaped by ideas + confidence meters**        |

---

> ğŸ’¡ **â€œThe best products come not from the best ideasâ€”but from the best systems to discover and validate ideas.â€**

If the **culture isnâ€™t aligned**, no framework will succeed. But if the **culture enables learning, experimentation, and curiosity**, great products become inevitable.

---

### ğŸ“„ Templates

#### ğŸ§© **XYZ Hypothesis Builder**

> â€œX people will do Y within Z time.â€

Use this to define your test goal:

* 100 developers will click â€œJoin Waitlistâ€ within 3 days.

#### ğŸ§ª **Market Engagement Hypothesis (MEH)**

> â€œIf we offer this, people will respond this wayâ€¦â€

Define the behavior you expect (click, sign up, pay).

#### ğŸ“Š **Initial Level of Interest (ILI) Tracker**

> Formula: **ILI = # who engage / # exposed**

Helps compare multiple experiments and **make data-informed Go/Kill decisions**.

---

### ğŸ§° Tools & Frameworks

| Need                     | Suggested Tools                     |
| ------------------------ | ----------------------------------- |
| **Landing Pages**        | Carrd, Webflow, Unbounce            |
| **Analytics & Heatmaps** | Google Analytics, Hotjar, Crazy Egg |
| **Form Builders**        | Tally, Typeform, Google Forms       |
| **Prototyping**          | Figma, InVision, Marvel             |
| **No-Code MVPs**         | Bubble, Glide, Adalo                |
| **A/B Testing**          | Google Optimize, Optimizely         |

> âœ… **â€œYou donâ€™t need codeâ€”you need curiosity and a clear hypothesis.â€**

---

### ğŸ“Š **Confidence Meter Template**

* Color-coded framework to visualize how much **evidence backs each idea**
* Helps reduce **political debates** and focus on facts

---

### ğŸ’¡ **Idea Bank Canvas**

* A **centralized repository** for tracking, scoring, and filtering ideas
* Encourages divergent thinking, then structured convergence

> âœ… â€œLet ideas competeâ€”based on value and evidence.â€

---

### ğŸ§ª **Step Project Tracker**

* Keeps all experiments **visible, documented, and reviewed**
* Tracks learnings, metrics, outcomes, and next steps

---

### ğŸ¯ **Goal Tracker**

* Links team OKRs or outcome goals to **ongoing experiments and ideas**
* Ensures alignment between **strategy and day-to-day execution**

> âœ… â€œYou canâ€™t scale impact if you donâ€™t track whatâ€™s working.â€


## ğŸ¯ Final Takeaway: Culture Eats Tools for Breakfast

> ğŸ”¥ **â€œTools donâ€™t drive innovationâ€”mindsets do.â€**

If your team:

* Experiments fast,
* Embraces micro-failures,
* Prioritizes behavioral validation,
* Kills bad ideas early and proudlyâ€¦

Then you are already ahead of 90% of startups and corporate innovation teams.

> ğŸ§  **â€œThe Right It is not just a productâ€”itâ€™s a habit of mind.â€**

---

# Quotes


# References
